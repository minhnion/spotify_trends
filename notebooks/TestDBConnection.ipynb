{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa6c8be2-10b4-4ceb-845a-12971193b12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dotenv\n",
      "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv, dotenv\n",
      "Successfully installed dotenv-0.9.9 python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42740c82-a420-4791-8547-52057462151b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.11/site-packages (2.9.11)\n"
     ]
    }
   ],
   "source": [
    "!pip install psycopg2-binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49ba076a-8a12-490a-b0b8-a48875ab584f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Khá»Ÿi táº¡o Spark (Batch Mode - Fixed FS)...\n",
      "âœ… Spark Ready!\n",
      "ğŸ” QuÃ©t file trong MinIO...\n",
      "ğŸ“‚ TÃ¬m tháº¥y 100 files JSON.\n",
      "ğŸ”„ [1/100] Processing: mpd.slice.114000-114999.json ... âœ… Done\n",
      "ğŸ”„ [2/100] Processing: mpd.slice.115000-115999.json ... âœ… Done\n",
      "ğŸ”„ [3/100] Processing: mpd.slice.117000-117999.json ... âœ… Done\n",
      "ğŸ”„ [4/100] Processing: mpd.slice.124000-124999.json ... âœ… Done\n",
      "ğŸ”„ [5/100] Processing: mpd.slice.126000-126999.json ... âœ… Done\n",
      "ğŸ”„ [6/100] Processing: mpd.slice.127000-127999.json ... âœ… Done\n",
      "ğŸ”„ [7/100] Processing: mpd.slice.191000-191999.json ... âœ… Done\n",
      "ğŸ”„ [8/100] Processing: mpd.slice.192000-192999.json ... âœ… Done\n",
      "ğŸ”„ [9/100] Processing: mpd.slice.193000-193999.json ... âœ… Done\n",
      "ğŸ”„ [10/100] Processing: mpd.slice.214000-214999.json ... âœ… Done\n",
      "ğŸ”„ [11/100] Processing: mpd.slice.216000-216999.json ... âœ… Done\n",
      "ğŸ”„ [12/100] Processing: mpd.slice.217000-217999.json ... âœ… Done\n",
      "ğŸ”„ [13/100] Processing: mpd.slice.22000-22999.json ... âœ… Done\n",
      "ğŸ”„ [14/100] Processing: mpd.slice.224000-224999.json ... âœ… Done\n",
      "ğŸ”„ [15/100] Processing: mpd.slice.225000-225999.json ... âœ… Done\n",
      "ğŸ”„ [16/100] Processing: mpd.slice.227000-227999.json ... âœ… Done\n",
      "ğŸ”„ [17/100] Processing: mpd.slice.290000-290999.json ... âœ… Done\n",
      "ğŸ”„ [18/100] Processing: mpd.slice.291000-291999.json ... âœ… Done\n",
      "ğŸ”„ [19/100] Processing: mpd.slice.292000-292999.json ... âœ… Done\n",
      "ğŸ”„ [20/100] Processing: mpd.slice.3000-3999.json ... âœ… Done\n",
      "ğŸ”„ [21/100] Processing: mpd.slice.315000-315999.json ... âœ… Done\n",
      "ğŸ”„ [22/100] Processing: mpd.slice.316000-316999.json ... âœ… Done\n",
      "ğŸ”„ [23/100] Processing: mpd.slice.317000-317999.json ... âœ… Done\n",
      "ğŸ”„ [24/100] Processing: mpd.slice.324000-324999.json ... âœ… Done\n",
      "ğŸ”„ [25/100] Processing: mpd.slice.325000-325999.json ... âœ… Done\n",
      "ğŸ”„ [26/100] Processing: mpd.slice.326000-326999.json ... âœ… Done\n",
      "ğŸ”„ [27/100] Processing: mpd.slice.38000-38999.json ... âœ… Done\n",
      "ğŸ”„ [28/100] Processing: mpd.slice.390000-390999.json ... âœ… Done\n",
      "ğŸ”„ [29/100] Processing: mpd.slice.391000-391999.json ... âœ… Done\n",
      "ğŸ”„ [30/100] Processing: mpd.slice.393000-393999.json ... âœ… Done\n",
      "ğŸ”„ [31/100] Processing: mpd.slice.410000-410999.json ... âœ… Done\n",
      "ğŸ”„ [32/100] Processing: mpd.slice.411000-411999.json ... âœ… Done\n",
      "ğŸ”„ [33/100] Processing: mpd.slice.413000-413999.json ... âœ… Done\n",
      "ğŸ”„ [34/100] Processing: mpd.slice.420000-420999.json ... âœ… Done\n",
      "ğŸ”„ [35/100] Processing: mpd.slice.422000-422999.json ... âœ… Done\n",
      "ğŸ”„ [36/100] Processing: mpd.slice.423000-423999.json ... âœ… Done\n",
      "ğŸ”„ [37/100] Processing: mpd.slice.448000-448999.json ... âœ… Done\n",
      "ğŸ”„ [38/100] Processing: mpd.slice.449000-449999.json ... âœ… Done\n",
      "ğŸ”„ [39/100] Processing: mpd.slice.479000-479999.json ... âœ… Done\n",
      "ğŸ”„ [40/100] Processing: mpd.slice.495000-495999.json ... âœ… Done\n",
      "ğŸ”„ [41/100] Processing: mpd.slice.496000-496999.json ... âœ… Done\n",
      "ğŸ”„ [42/100] Processing: mpd.slice.497000-497999.json ... âœ… Done\n",
      "ğŸ”„ [43/100] Processing: mpd.slice.51000-51999.json ... âœ… Done\n",
      "ğŸ”„ [44/100] Processing: mpd.slice.510000-510999.json ... âœ… Done\n",
      "ğŸ”„ [45/100] Processing: mpd.slice.511000-511999.json ... âœ… Done\n",
      "ğŸ”„ [46/100] Processing: mpd.slice.512000-512999.json ... âœ… Done\n",
      "ğŸ”„ [47/100] Processing: mpd.slice.521000-521999.json ... âœ… Done\n",
      "ğŸ”„ [48/100] Processing: mpd.slice.522000-522999.json ... âœ… Done\n",
      "ğŸ”„ [49/100] Processing: mpd.slice.523000-523999.json ... âœ… Done\n",
      "ğŸ”„ [50/100] Processing: mpd.slice.54000-54999.json ... âœ… Done\n",
      "ğŸ”„ [51/100] Processing: mpd.slice.548000-548999.json ... âœ… Done\n",
      "ğŸ”„ [52/100] Processing: mpd.slice.549000-549999.json ... âœ… Done\n",
      "ğŸ”„ [53/100] Processing: mpd.slice.578000-578999.json ... âœ… Done\n",
      "ğŸ”„ [54/100] Processing: mpd.slice.594000-594999.json ... âœ… Done\n",
      "ğŸ”„ [55/100] Processing: mpd.slice.596000-596999.json ... âœ… Done\n",
      "ğŸ”„ [56/100] Processing: mpd.slice.597000-597999.json ... âœ… Done\n",
      "ğŸ”„ [57/100] Processing: mpd.slice.611000-611999.json ... âœ… Done\n",
      "ğŸ”„ [58/100] Processing: mpd.slice.612000-612999.json ... âœ… Done\n",
      "ğŸ”„ [59/100] Processing: mpd.slice.613000-613999.json ... âœ… Done\n",
      "ğŸ”„ [60/100] Processing: mpd.slice.620000-620999.json ... âœ… Done\n",
      "ğŸ”„ [61/100] Processing: mpd.slice.621000-621999.json ... âœ… Done\n",
      "ğŸ”„ [62/100] Processing: mpd.slice.622000-622999.json ... âœ… Done\n",
      "ğŸ”„ [63/100] Processing: mpd.slice.678000-678999.json ... âœ… Done\n",
      "ğŸ”„ [64/100] Processing: mpd.slice.679000-679999.json ... âœ… Done\n",
      "ğŸ”„ [65/100] Processing: mpd.slice.694000-694999.json ... âœ… Done\n",
      "ğŸ”„ [66/100] Processing: mpd.slice.695000-695999.json ... âœ… Done\n",
      "ğŸ”„ [67/100] Processing: mpd.slice.697000-697999.json ... âœ… Done\n",
      "ğŸ”„ [68/100] Processing: mpd.slice.7000-7999.json ... âœ… Done\n",
      "ğŸ”„ [69/100] Processing: mpd.slice.70000-70999.json ... âœ… Done\n",
      "ğŸ”„ [70/100] Processing: mpd.slice.710000-710999.json ... âœ… Done\n",
      "ğŸ”„ [71/100] Processing: mpd.slice.712000-712999.json ... âœ… Done\n",
      "ğŸ”„ [72/100] Processing: mpd.slice.713000-713999.json ... âœ… Done\n",
      "ğŸ”„ [73/100] Processing: mpd.slice.720000-720999.json ... âœ… Done\n",
      "ğŸ”„ [74/100] Processing: mpd.slice.721000-721999.json ... âœ… Done\n",
      "ğŸ”„ [75/100] Processing: mpd.slice.723000-723999.json ... âœ… Done\n",
      "ğŸ”„ [76/100] Processing: mpd.slice.749000-749999.json ... âœ… Done\n",
      "ğŸ”„ [77/100] Processing: mpd.slice.75000-75999.json ... âœ… Done\n",
      "ğŸ”„ [78/100] Processing: mpd.slice.778000-778999.json ... âœ… Done\n",
      "ğŸ”„ [79/100] Processing: mpd.slice.779000-779999.json ... âœ… Done\n",
      "ğŸ”„ [80/100] Processing: mpd.slice.794000-794999.json ... âœ… Done\n",
      "ğŸ”„ [81/100] Processing: mpd.slice.795000-795999.json ... âœ… Done\n",
      "ğŸ”„ [82/100] Processing: mpd.slice.796000-796999.json ... âœ… Done\n",
      "ğŸ”„ [83/100] Processing: mpd.slice.81000-81999.json ... âœ… Done\n",
      "ğŸ”„ [84/100] Processing: mpd.slice.84000-84999.json ... âœ… Done\n",
      "ğŸ”„ [85/100] Processing: mpd.slice.844000-844999.json ... âœ… Done\n",
      "ğŸ”„ [86/100] Processing: mpd.slice.846000-846999.json ... âœ… Done\n",
      "ğŸ”„ [87/100] Processing: mpd.slice.847000-847999.json ... âœ… Done\n",
      "ğŸ”„ [88/100] Processing: mpd.slice.874000-874999.json ... âœ… Done\n",
      "ğŸ”„ [89/100] Processing: mpd.slice.875000-875999.json ... âœ… Done\n",
      "ğŸ”„ [90/100] Processing: mpd.slice.877000-877999.json ... âœ… Done\n",
      "ğŸ”„ [91/100] Processing: mpd.slice.898000-898999.json ... âœ… Done\n",
      "ğŸ”„ [92/100] Processing: mpd.slice.899000-899999.json ... âœ… Done\n",
      "ğŸ”„ [93/100] Processing: mpd.slice.945000-945999.json ... âœ… Done\n",
      "ğŸ”„ [94/100] Processing: mpd.slice.946000-946999.json ... âœ… Done\n",
      "ğŸ”„ [95/100] Processing: mpd.slice.947000-947999.json ... âœ… Done\n",
      "ğŸ”„ [96/100] Processing: mpd.slice.974000-974999.json ... âœ… Done\n",
      "ğŸ”„ [97/100] Processing: mpd.slice.975000-975999.json ... âœ… Done\n",
      "ğŸ”„ [98/100] Processing: mpd.slice.976000-976999.json ... âœ… Done\n",
      "ğŸ”„ [99/100] Processing: mpd.slice.998000-998999.json ... âœ… Done\n",
      "ğŸ”„ [100/100] Processing: mpd.slice.999000-999999.json ... âœ… Done\n",
      "\n",
      "ğŸ‰ BATCH PROCESSING COMPLETED!\n",
      "ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ lÆ°u: 6615180\n",
      "root\n",
      " |-- playlist_id: long (nullable = true)\n",
      " |-- playlist_name: string (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- artist_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- album_uri: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, current_timestamp\n",
    "\n",
    "# --- 1. KHá»I Táº O SPARK ---\n",
    "JAR_PATH = \"/home/jovyan/jars\"\n",
    "jars = [f\"{JAR_PATH}/hadoop-aws-3.3.4.jar\", f\"{JAR_PATH}/aws-java-sdk-bundle-1.12.262.jar\"]\n",
    "\n",
    "print(\"ğŸš€ Khá»Ÿi táº¡o Spark (Batch Mode - Fixed FS)...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Safe Batch\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"âœ… Spark Ready!\")\n",
    "\n",
    "# --- 2. Láº¤Y DANH SÃCH FILE Tá»ª S3 (FIX Lá»–I WRONG FS) ---\n",
    "print(\"ğŸ” QuÃ©t file trong MinIO...\")\n",
    "\n",
    "# Láº¥y cÃ¡c class Java cáº§n thiáº¿t\n",
    "Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "fs_conf = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "# 1. Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n\n",
    "bucket_path = Path(\"s3a://spotify-raw-data/\")\n",
    "\n",
    "# 2. Láº¥y FileSystem TÆ¯Æ NG á»¨NG vá»›i Ä‘Æ°á»ng dáº«n Ä‘Ã³ (Thay vÃ¬ láº¥y máº·c Ä‘á»‹nh)\n",
    "try:\n",
    "    fs = bucket_path.getFileSystem(fs_conf) # <--- Sá»¬A Lá»–I Táº I ÄÃ‚Y\n",
    "    \n",
    "    # 3. Liá»‡t kÃª file\n",
    "    file_statuses = fs.listStatus(bucket_path)\n",
    "    s3_files = [f.getPath().toString() for f in file_statuses if f.getPath().getName().endswith('.json')]\n",
    "    s3_files.sort()\n",
    "    \n",
    "    print(f\"ğŸ“‚ TÃ¬m tháº¥y {len(s3_files)} files JSON.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Lá»—i liá»‡t kÃª file: {e}\")\n",
    "    s3_files = []\n",
    "\n",
    "# --- 3. VÃ’NG Láº¶P Xá»¬ LÃ (QUAN TRá»ŒNG) ---\n",
    "output_path = \"s3a://warehouse/spotify_full_schema\"\n",
    "count = 0\n",
    "\n",
    "for file_path in s3_files:\n",
    "    count += 1\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    print(f\"ğŸ”„ [{count}/{len(s3_files)}] Processing: {file_name} ...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # Äá»c 1 file\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "        \n",
    "        if \"playlists\" in df.columns:\n",
    "            # Transform\n",
    "            tracks_df = df.select(explode(col(\"playlists\")).alias(\"p\")) \\\n",
    "                .select(\n",
    "                    col(\"p.pid\").alias(\"playlist_id\"),\n",
    "                    col(\"p.name\").alias(\"playlist_name\"),\n",
    "                    explode(col(\"p.tracks\")).alias(\"t\")\n",
    "                ).select(\n",
    "                    col(\"playlist_id\"), \n",
    "                    col(\"playlist_name\"),\n",
    "                    col(\"t.track_uri\"),\n",
    "                    col(\"t.track_name\"),\n",
    "                    col(\"t.artist_uri\"),\n",
    "                    col(\"t.artist_name\"),\n",
    "                    col(\"t.album_uri\"),\n",
    "                    col(\"t.album_name\"), \n",
    "                    col(\"t.duration_ms\")\n",
    "                )\n",
    "            \n",
    "            # Write (Mode: Append)\n",
    "            # File Ä‘áº§u tiÃªn overwrite Ä‘á»ƒ xÃ³a cÅ©, cÃ¡c file sau append\n",
    "            mode = \"overwrite\" if count == 1 else \"append\"\n",
    "            tracks_df.write.mode(mode).parquet(output_path)\n",
    "            \n",
    "            # Giáº£i phÃ³ng RAM ngay láº­p tá»©c\n",
    "            df.unpersist()\n",
    "            tracks_df.unpersist()\n",
    "            print(\"âœ… Done\")\n",
    "        else:\n",
    "            print(\"âš ï¸ Skipped (No data)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fail: {e}\")\n",
    "        # Náº¿u gáº·p lá»—i Java Heap Space thÃ¬ dá»«ng luÃ´n vÃ²ng láº·p Ä‘á»ƒ báº£o vá»‡ mÃ¡y\n",
    "        if \"Java heap space\" in str(e):\n",
    "            print(\"ğŸ’€ STOPPING DUE TO MEMORY LIMIT\")\n",
    "            break\n",
    "\n",
    "print(\"\\nğŸ‰ BATCH PROCESSING COMPLETED!\")\n",
    "\n",
    "# Kiá»ƒm tra káº¿t quáº£\n",
    "try:\n",
    "    final_df = spark.read.parquet(output_path)\n",
    "    print(f\"ğŸ“Š Tá»•ng sá»‘ dÃ²ng Ä‘Ã£ lÆ°u: {final_df.count()}\")\n",
    "    final_df.printSchema()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74a5856b-7c16-406d-98e1-38c2c7028703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Khá»Ÿi táº¡o Spark vá»›i Postgres Driver...\n",
      "âœ… Spark Session Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# ÄÆ°á»ng dáº«n trong Docker\n",
    "JAR_PATH = \"/home/jovyan/jars\"\n",
    "\n",
    "# Danh sÃ¡ch JARs (Pháº£i cÃ³ Ä‘á»§ 3 file nÃ y)\n",
    "jars = [\n",
    "    f\"{JAR_PATH}/hadoop-aws-3.3.4.jar\",\n",
    "    f\"{JAR_PATH}/aws-java-sdk-bundle-1.12.262.jar\",\n",
    "    f\"{JAR_PATH}/postgresql-42.6.0.jar\"  # <--- Báº®T BUá»˜C PHáº¢I CÃ“ DÃ’NG NÃ€Y\n",
    "]\n",
    "\n",
    "print(\"ğŸš€ Khá»Ÿi táº¡o Spark vá»›i Postgres Driver...\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify DB Population\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"âœ… Spark Session Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b8d643-d7aa-4377-95f0-aa50f7fa97ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ Äang cáº¥u hÃ¬nh láº¡i Database...\n",
      "âœ… ÄÃ£ Reset Schema báº£ng 'playlist_tracks' thÃ nh cÃ´ng!\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "from urllib.parse import urlparse, unquote\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# --- Cáº¤U HÃŒNH ---\n",
    "SUPABASE_URL = \"postgresql://postgres.xucnhlecthkbalouzymg:Nguyenminh21082004@aws-1-ap-southeast-1.pooler.supabase.com:6543/postgres\"\n",
    "parsed = urlparse(SUPABASE_URL)\n",
    "\n",
    "jdbc_url = f\"jdbc:postgresql://{parsed.hostname}:{parsed.port}{parsed.path}?prepareThreshold=0\"\n",
    "\n",
    "db_properties = {\n",
    "    \"user\": parsed.username,\n",
    "    \"password\": unquote(parsed.password),\n",
    "    \"driver\": \"org.postgresql.Driver\",\n",
    "    \"sslmode\": \"require\"\n",
    "}\n",
    "\n",
    "# --- PHáº¦N 1: Dá»ŒN Dáº¸P & TÃI Cáº¤U TRÃšC DB (FIX Lá»–I DATABASE SCHEMA) ---\n",
    "print(\"ğŸ› ï¸ Äang cáº¥u hÃ¬nh láº¡i Database...\")\n",
    "try:\n",
    "    conn = psycopg2.connect(\n",
    "        host=parsed.hostname, port=parsed.port, database=\"postgres\",\n",
    "        user=parsed.username, password=unquote(parsed.password), sslmode='require'\n",
    "    )\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    # 1. XÃ³a báº£ng con bá»‹ sai schema trÆ°á»›c\n",
    "    cur.execute(\"DROP TABLE IF EXISTS playlist_tracks CASCADE;\")\n",
    "    \n",
    "    # 2. XÃ³a dá»¯ liá»‡u cÃ¡c báº£ng cha (giá»¯ vá» báº£ng náº¿u Ä‘Ã£ Ä‘Ãºng)\n",
    "    cur.execute(\"TRUNCATE TABLE tracks, playlists, albums, artists CASCADE;\")\n",
    "    \n",
    "    # 3. Táº O Láº I báº£ng playlist_tracks vá»›i tÃªn cá»™t CHUáº¨N (pid, track_uri)\n",
    "    # ÄÃ¢y lÃ  bÆ°á»›c quan trá»ng nháº¥t Ä‘á»ƒ sá»­a lá»—i cá»§a báº¡n\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS playlist_tracks (\n",
    "        pid INTEGER, -- LÆ°u Ã½: Spark LongType mapping sang Ä‘Ã¢y ok\n",
    "        track_uri VARCHAR(255),\n",
    "        PRIMARY KEY (pid, track_uri)\n",
    "    );\n",
    "    \"\"\"\n",
    "    cur.execute(create_table_sql)\n",
    "    \n",
    "    conn.commit()\n",
    "    print(\"âœ… ÄÃ£ Reset Schema báº£ng 'playlist_tracks' thÃ nh cÃ´ng!\")\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Lá»—i SQL: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4a7459b-8b23-4153-9d19-14fbe4713bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Äang Ä‘á»c dá»¯ liá»‡u Parquet...\n",
      "ğŸ’¾ Äang ghi báº£ng 'artists' (110329 dÃ²ng)...\n",
      "   âœ… Xong báº£ng artists\n",
      "ğŸ’¾ Äang ghi báº£ng 'albums' (272997 dÃ²ng)...\n",
      "   âœ… Xong báº£ng albums\n",
      "ğŸ’¾ Äang ghi báº£ng 'playlists' (100000 dÃ²ng)...\n",
      "   âœ… Xong báº£ng playlists\n",
      "ğŸ’¾ Äang ghi báº£ng 'tracks' (682955 dÃ²ng)...\n",
      "   âœ… Xong báº£ng tracks\n",
      "\n",
      "=== Processing Playlist_Tracks Table ===\n",
      "Schema khá»›p vá»›i Database:\n",
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      "\n",
      "ğŸ’¾ Äang ghi báº£ng 'playlist_tracks' (6527816 dÃ²ng)...\n",
      "   âœ… Xong báº£ng playlist_tracks\n",
      "\n",
      "ğŸ‰ CHÃšC Má»ªNG! Dá»¯ liá»‡u Ä‘Ã£ lÃªn mÃ¢y thÃ nh cÃ´ng!\n"
     ]
    }
   ],
   "source": [
    "# --- PHáº¦N 2: GHI Dá»® LIá»†U Má»šI ---\n",
    "print(\"â³ Äang Ä‘á»c dá»¯ liá»‡u Parquet...\")\n",
    "full_df = spark.read.parquet(\"s3a://warehouse/spotify_full_schema\")\n",
    "\n",
    "# HÃ m ghi tiá»‡n Ã­ch\n",
    "def write_to_pg(df, table_name):\n",
    "    print(f\"ğŸ’¾ Äang ghi báº£ng '{table_name}' ({df.count()} dÃ²ng)...\")\n",
    "    # Mode append: Ghi vÃ o báº£ng Ä‘Ã£ táº¡o sáºµn á»Ÿ trÃªn\n",
    "    df.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=db_properties)\n",
    "    print(f\"   âœ… Xong báº£ng {table_name}\")\n",
    "\n",
    "try:\n",
    "    # 1. Báº£ng ARTISTS\n",
    "    artists = full_df.select(\"artist_uri\", \"artist_name\").distinct()\n",
    "    write_to_pg(artists, \"artists\")\n",
    "\n",
    "    # 2. Báº£ng ALBUMS\n",
    "    albums = full_df.select(\"album_uri\", \"album_name\").distinct()\n",
    "    write_to_pg(albums, \"albums\")\n",
    "\n",
    "    # 3. Báº£ng PLAYLISTS\n",
    "    playlists = full_df.select(\n",
    "        col(\"playlist_id\").alias(\"pid\"), \n",
    "        col(\"playlist_name\")\n",
    "    ).distinct()\n",
    "    write_to_pg(playlists, \"playlists\")\n",
    "\n",
    "    # 4. Báº£ng TRACKS\n",
    "    tracks = full_df.select(\n",
    "        \"track_uri\", \"track_name\", \"artist_uri\", \"album_uri\", \"duration_ms\"\n",
    "    ).distinct()\n",
    "    write_to_pg(tracks, \"tracks\")\n",
    "\n",
    "    # 5. Báº£ng PLAYLIST_TRACKS (QUAN TRá»ŒNG)\n",
    "    print(\"\\n=== Processing Playlist_Tracks Table ===\")\n",
    "    \n",
    "    # Äá»•i tÃªn biáº¿n Ä‘á»ƒ trÃ¡nh cache cÅ©\n",
    "    df_final_link = full_df.select(\n",
    "        col(\"playlist_id\").alias(\"pid\"), \n",
    "        col(\"track_uri\")\n",
    "    ).distinct()\n",
    "    \n",
    "    # Debug schema láº§n cuá»‘i\n",
    "    print(\"Schema khá»›p vá»›i Database:\")\n",
    "    df_final_link.printSchema()\n",
    "    \n",
    "    write_to_pg(df_final_link, \"playlist_tracks\")\n",
    "\n",
    "    print(\"\\nğŸ‰ CHÃšC Má»ªNG! Dá»¯ liá»‡u Ä‘Ã£ lÃªn mÃ¢y thÃ nh cÃ´ng!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Lá»—i khi ghi DB: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d145d6-8c39-4141-aafa-6e2865c0ea2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
