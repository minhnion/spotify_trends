{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108f97fd-84fc-43a7-9b20-2afea526a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (2.31.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /opt/conda/lib/python3.11/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests) (2023.7.22)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dae9c39-9e78-4091-b16f-c84932467428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing the MinIO server or file: HTTPConnectionPool(host='localhost', port=9001): Max retries exceeded with url: /browser/spotify-sample/mpd.slice.0-999.json (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f96bcf16810>: Failed to establish a new connection: [Errno 111] Connection refused'))\n",
      "Ensure MinIO is running and accessible at http://localhost:9001.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# The direct URL to the JSON file on your MinIO console\n",
    "DATA_URL = 'http://localhost:9001/browser/spotify-sample/mpd.slice.0-999.json'\n",
    "\n",
    "try:\n",
    "    # 1. Fetch the data from the URL\n",
    "    response = requests.get(DATA_URL)\n",
    "    response.raise_for_status() # Check for HTTP errors (4xx or 5xx)\n",
    "\n",
    "    # 2. Parse the JSON content\n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"Data successfully fetched and parsed!\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error accessing the MinIO server or file: {e}\")\n",
    "    print(\"Ensure MinIO is running and accessible at http://localhost:9001.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    print(\"The file might not be valid JSON.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4440c498-42d1-41df-b3f5-3e74819498c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /opt/conda/lib/python3.11/site-packages (7.2.18)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.11/site-packages (from minio) (23.1.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.11/site-packages (from minio) (2023.7.22)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.11/site-packages (from minio) (3.23.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.11/site-packages (from minio) (4.8.0)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.11/site-packages (from minio) (2.0.7)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.11/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.11/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.11/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35e53a3a-c79a-46a0-8539-02883ae4797c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'minio'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mminio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Minio\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Khá»Ÿi táº¡o client\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'minio'"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "import json\n",
    "\n",
    "# Khá»Ÿi táº¡o client\n",
    "client = Minio(\"host.docker.internal:9000\", access_key=\"minioadmin\", secret_key=\"minioadmin\", secure=False)\n",
    "\n",
    "# Táº£i file JSON tá»« bucket\n",
    "data = client.get_object(\"spotify-sample\", \"mpd.slice.0-999.json\")\n",
    "\n",
    "# Äá»c ná»™i dung JSON\n",
    "playlist_data = json.load(data)\n",
    "print(len(playlist_data[\"playlists\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb8b296-1bda-4186-ae72-ad124dbde320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Sá»‘ lÆ°á»£ng playlist: 1000\n",
      "âœ… Tá»•ng sá»‘ track entries: 67503\n",
      "âœ… Sá»‘ lÆ°á»£ng track duy nháº¥t: 30049\n",
      "   playlist_id playlist_name                                  track_name  \\\n",
      "0            0    Throwbacks  Lose Control (feat. Ciara & Fat Man Scoop)   \n",
      "1            0    Throwbacks                                       Toxic   \n",
      "2            0    Throwbacks                               Crazy In Love   \n",
      "3            0    Throwbacks                              Rock Your Body   \n",
      "4            0    Throwbacks                                It Wasn't Me   \n",
      "\n",
      "         artist_name                                    album_name  \n",
      "0      Missy Elliott                                  The Cookbook  \n",
      "1     Britney Spears                                   In The Zone  \n",
      "2            BeyoncÃ©  Dangerously In Love (Alben fÃ¼r die Ewigkeit)  \n",
      "3  Justin Timberlake                                     Justified  \n",
      "4             Shaggy                                      Hot Shot  \n"
     ]
    }
   ],
   "source": [
    "playlists = playlist_data['playlists']\n",
    "records = []\n",
    "\n",
    "for p in playlists:\n",
    "    for t in p['tracks']:\n",
    "        records.append({\n",
    "            'playlist_id': p['pid'],\n",
    "            'playlist_name': p.get('name', ''),\n",
    "            'track_name': t.get('track_name', ''),\n",
    "            'artist_name': t.get('artist_name', ''),\n",
    "            'album_name': t.get('album_name', '')\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(\"âœ… Sá»‘ lÆ°á»£ng playlist:\", df['playlist_id'].nunique())\n",
    "print(\"âœ… Tá»•ng sá»‘ track entries:\", len(df))\n",
    "print(\"âœ… Sá»‘ lÆ°á»£ng track duy nháº¥t:\", df['track_name'].nunique())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8088929-ccde-4100-93b2-e76737809ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ bÃ i hÃ¡t trung bÃ¬nh má»—i playlist: 67.50\n",
      "\n",
      "Top 10 nghá»‡ sÄ© phá»• biáº¿n nháº¥t:\n",
      "artist_name\n",
      "Drake             939\n",
      "Kanye West        415\n",
      "Kendrick Lamar    385\n",
      "Rihanna           350\n",
      "Eminem            332\n",
      "The Weeknd        296\n",
      "Lil Uzi Vert      292\n",
      "Ed Sheeran        285\n",
      "Future            265\n",
      "Chris Brown       259\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 album phá»• biáº¿n nháº¥t:\n",
      "album_name\n",
      "Views                        234\n",
      "Stoney                       184\n",
      "DAMN.                        152\n",
      "Greatest Hits                149\n",
      "Beauty Behind The Madness    146\n",
      "Original Album Classics      138\n",
      "Blurryface                   134\n",
      "More Life                    127\n",
      "American Teen                122\n",
      "The Life Of Pablo            121\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avg_tracks = df.groupby('playlist_id')['track_name'].count().mean()\n",
    "print(f\"Sá»‘ bÃ i hÃ¡t trung bÃ¬nh má»—i playlist: {avg_tracks:.2f}\")\n",
    "\n",
    "# Nghá»‡ sÄ© phá»• biáº¿n nháº¥t\n",
    "top_artists = df['artist_name'].value_counts().head(10)\n",
    "print(\"\\nTop 10 nghá»‡ sÄ© phá»• biáº¿n nháº¥t:\")\n",
    "print(top_artists)\n",
    "\n",
    "# Album phá»• biáº¿n nháº¥t\n",
    "top_albums = df['album_name'].value_counts().head(10)\n",
    "print(\"\\nTop 10 album phá»• biáº¿n nháº¥t:\")\n",
    "print(top_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24573f06-23bf-46d7-89b0-ae63f5660f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Cáº¥u hÃ¬nh chuáº©n Ä‘á»ƒ cháº¡y vá»›i MinIO trong Docker\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Production Pipeline\") \\\n",
    "    .config(\"spark.jars\", \"/home/jovyan/jars/hadoop-aws-3.3.4.jar,/home/jovyan/jars/aws-java-sdk-bundle-1.12.262.jar\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f118717-b707-4d73-94e9-46ba501b6ca7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'StructType' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Äá»‹nh nghÄ©a Schema chuáº©n cá»§a Spotify MPD\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Viá»‡c nÃ y giÃºp Spark Ä‘á»c nhanh hÆ¡n vÃ  loáº¡i bá» báº£n ghi sai format ngay tá»« Ä‘áº§u\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m track_schema \u001b[38;5;241m=\u001b[39m \u001b[43mStructType\u001b[49m([\n\u001b[1;32m      4\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m\"\u001b[39m, IntegerType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martist_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      7\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124martist_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      8\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbum_uri\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mduration_ms\u001b[39m\u001b[38;5;124m\"\u001b[39m, LongType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     11\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malbum_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m ])\n\u001b[1;32m     14\u001b[0m playlist_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     15\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcollaborative\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m ])\n\u001b[1;32m     26\u001b[0m root_schema \u001b[38;5;241m=\u001b[39m StructType([\n\u001b[1;32m     27\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m\"\u001b[39m, MapType(StringType(), StringType()), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m     28\u001b[0m     StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplaylists\u001b[39m\u001b[38;5;124m\"\u001b[39m, ArrayType(playlist_schema), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     29\u001b[0m ])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'StructType' is not defined"
     ]
    }
   ],
   "source": [
    "# 1. Äá»‹nh nghÄ©a Schema chuáº©n cá»§a Spotify MPD\n",
    "# Viá»‡c nÃ y giÃºp Spark Ä‘á»c nhanh hÆ¡n vÃ  loáº¡i bá» báº£n ghi sai format ngay tá»« Ä‘áº§u\n",
    "track_schema = StructType([\n",
    "    StructField(\"pos\", IntegerType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"track_uri\", StringType(), True),\n",
    "    StructField(\"artist_uri\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True),\n",
    "    StructField(\"album_uri\", StringType(), True),\n",
    "    StructField(\"duration_ms\", LongType(), True),\n",
    "    StructField(\"album_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "playlist_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"collaborative\", StringType(), True),\n",
    "    StructField(\"pid\", LongType(), True),\n",
    "    StructField(\"modified_at\", LongType(), True),\n",
    "    StructField(\"num_tracks\", IntegerType(), True),\n",
    "    StructField(\"num_albums\", IntegerType(), True),\n",
    "    StructField(\"num_followers\", IntegerType(), True),\n",
    "    StructField(\"tracks\", ArrayType(track_schema), True), # Nested Array\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "\n",
    "root_schema = StructType([\n",
    "    StructField(\"info\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"playlists\", ArrayType(playlist_schema), True)\n",
    "])\n",
    "\n",
    "print(\"Loading Raw Data from Data Lake...\")\n",
    "# Äá»c dá»¯ liá»‡u vá»›i schema Ä‘Ã£ Ä‘á»‹nh nghÄ©a\n",
    "raw_df = spark.read \\\n",
    "    .schema(root_schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"s3a://spotify-raw-data/mpd.slice.0-999.json\") # Demo 1 file\n",
    "\n",
    "print(f\"Raw Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1529020b-6e9e-403a-9e8d-ba250c340d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Processing Silver Layer...\n",
      "âœ… Silver Layer (Cleaned & Normalized) Saved.\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "|playlist_id|   playlist_name|num_tracks|num_followers|      modified_date|           track_uri|      track_name|          artist_uri|        artist_name|duration_ms|artist_partition|\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "|          0|      Throwbacks|        52|            1|2017-04-29 00:00:00|spotify:track:2eJ...|            Baby|spotify:artist:1u...|      Justin Bieber|     213973|               J|\n",
      "|          1|Awesome Playlist|        39|            1|2017-09-28 00:00:00|spotify:track:2HH...|Eye of the Tiger|spotify:artist:26...|           Survivor|     243773|               S|\n",
      "|          1|Awesome Playlist|        39|            1|2017-09-28 00:00:00|spotify:track:3nJ...|  Right Hand Man|spotify:artist:6s...|Christopher Jackson|     321696|               C|\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Silver Layer...\")\n",
    "\n",
    "# 1. Explode Playlists (TÃ¡ch máº£ng playlist thÃ nh tá»«ng dÃ²ng)\n",
    "playlists_df = raw_df.select(explode(col(\"playlists\")).alias(\"p\"))\n",
    "\n",
    "# 2. Explode Tracks (TÃ¡ch máº£ng tracks lá»“ng bÃªn trong playlist)\n",
    "# ÄÃ¢y lÃ  bÆ°á»›c \"Explode nested track arrays\" trong yÃªu cáº§u\n",
    "flat_df = playlists_df.select(\n",
    "    col(\"p.pid\").alias(\"playlist_id\"),\n",
    "    col(\"p.name\").alias(\"playlist_name\"),\n",
    "    col(\"p.num_tracks\"),\n",
    "    col(\"p.num_followers\"),\n",
    "    col(\"p.modified_at\"),\n",
    "    explode(col(\"p.tracks\")).alias(\"t\")\n",
    ")\n",
    "\n",
    "# 3. Parse & Normalize Metadata (LÃ m sáº¡ch)\n",
    "# - Trim: Cáº¯t khoáº£ng tráº¯ng thá»«a\n",
    "# - Lower: Chuyá»ƒn vá» chá»¯ thÆ°á»ng Ä‘á»ƒ chuáº©n hÃ³a\n",
    "# - Cast: Äáº£m báº£o Ä‘Ãºng kiá»ƒu dá»¯ liá»‡u\n",
    "silver_df = flat_df.select(\n",
    "    col(\"playlist_id\"),\n",
    "    trim(col(\"playlist_name\")).alias(\"playlist_name\"),\n",
    "    col(\"num_tracks\"),\n",
    "    col(\"num_followers\"),\n",
    "    # Convert timestamp (modified_at lÃ  epoch seconds)\n",
    "    to_timestamp(col(\"modified_at\")).alias(\"modified_date\"),\n",
    "    \n",
    "    # Track Metadata Parsing\n",
    "    trim(col(\"t.track_uri\")).alias(\"track_uri\"),\n",
    "    trim(col(\"t.track_name\")).alias(\"track_name\"),\n",
    "    trim(col(\"t.artist_uri\")).alias(\"artist_uri\"),\n",
    "    trim(col(\"t.artist_name\")).alias(\"artist_name\"),\n",
    "    col(\"t.duration_ms\"),\n",
    "    \n",
    "    # ThÃªm cá»™t partition key (VÃ­ dá»¥: láº¥y chá»¯ cÃ¡i Ä‘áº§u cá»§a artist Ä‘á»ƒ partition cho Ä‘á»u)\n",
    "    substring(col(\"t.artist_name\"), 0, 1).alias(\"artist_partition\")\n",
    ").drop_duplicates() # Loáº¡i bá» dÃ²ng trÃ¹ng láº·p (Deduplication)\n",
    "\n",
    "# LÆ°u báº£ng sáº¡ch nÃ y xuá»‘ng Data Lake (Silver)\n",
    "silver_df.write.mode(\"overwrite\").parquet(\"s3a://warehouse/silver/tracks\")\n",
    "print(\"Silver Layer (Cleaned & Normalized) Saved.\")\n",
    "silver_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebfe64d1-9d2d-402f-979e-f62e020a2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gold Layer (Feature Engineering)...\n",
      "Sample Artist Features:\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "|     artist_name|total_playlist_appearances|total_tracks_count|avg_track_duration|total_listen_time_ms|  popularity_score|\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "|           Drake|                       202|               923|237832.82665222103|           219519699|418.29999999999995|\n",
      "|         Rihanna|                       170|               348| 224016.5459770115|            77957758|223.39999999999998|\n",
      "|      Kanye West|                       149|               412|248709.39077669903|           102468269|227.89999999999998|\n",
      "|      The Weeknd|                       139|               291| 264382.6632302406|            76935355|             184.6|\n",
      "|The Chainsmokers|                       121|               229|223965.34934497817|            51288065|153.39999999999998|\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sample Playlist Features:\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "|playlist_id|       playlist_name|playlist_length|unique_artists|avg_track_duration|diversity_ratio|\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "|        496|songs to sing in ...|             20|            20|          221572.7|            1.0|\n",
      "|        795|        Shape of You|              9|             9|212679.33333333334|            1.0|\n",
      "|        461|         BUST A MOVE|             18|            18|243411.44444444444|            1.0|\n",
      "|        922|         Baby shower|             12|            12|206043.83333333334|            1.0|\n",
      "|        641|               sleep|              8|             8|          274936.5|            1.0|\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Gold Layer (Feature Engineering)...\")\n",
    "\n",
    "# Feature 1: Artist Frequency (Äá»™ phá»• biáº¿n cá»§a nghá»‡ sÄ©)\n",
    "# Nghá»‡ sÄ© nÃ y xuáº¥t hiá»‡n trong bao nhiÃªu playlist? Tá»•ng thá»i lÆ°á»£ng nghe lÃ  bao nhiÃªu?\n",
    "artist_features = silver_df.groupBy(\"artist_name\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"playlist_id\").alias(\"total_playlist_appearances\"),\n",
    "        count(\"track_uri\").alias(\"total_tracks_count\"),\n",
    "        avg(\"duration_ms\").alias(\"avg_track_duration\"),\n",
    "        sum(\"duration_ms\").alias(\"total_listen_time_ms\")\n",
    "    ) \\\n",
    "    .withColumn(\"popularity_score\", col(\"total_playlist_appearances\") * 0.7 + col(\"total_tracks_count\") * 0.3)\n",
    "\n",
    "# Feature 2: Playlist Complexity (Äá»™ phá»©c táº¡p cá»§a Playlist)\n",
    "# Playlist nÃ y cÃ³ bao nhiÃªu nghá»‡ sÄ© khÃ¡c nhau? Äá»™ Ä‘a dáº¡ng tháº¿ nÃ o?\n",
    "playlist_features = silver_df.groupBy(\"playlist_id\", \"playlist_name\") \\\n",
    "    .agg(\n",
    "        count(\"track_uri\").alias(\"playlist_length\"),\n",
    "        countDistinct(\"artist_name\").alias(\"unique_artists\"),\n",
    "        avg(\"duration_ms\").alias(\"avg_track_duration\")\n",
    "    ) \\\n",
    "    .withColumn(\"diversity_ratio\", col(\"unique_artists\") / col(\"playlist_length\"))\n",
    "\n",
    "print(\"Sample Artist Features:\")\n",
    "artist_features.orderBy(desc(\"total_playlist_appearances\")).show(5)\n",
    "\n",
    "print(\"Sample Playlist Features:\")\n",
    "playlist_features.orderBy(desc(\"diversity_ratio\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae07444-927e-48e4-b642-81bca65671b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Final Data (Storage Optimization)...\n",
      "All Data Saved & Partitioned Successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Final Data (Storage Optimization)...\")\n",
    "\n",
    "# 1. LÆ°u Artist Features (Partition theo chá»¯ cÃ¡i Ä‘áº§u Ä‘á»ƒ truy váº¥n nhanh)\n",
    "# LÆ°u Ã½: Partition by artist_id/name trá»±c tiáº¿p sáº½ táº¡o ra hÃ ng triá»‡u folder nhá» (Bad Practice).\n",
    "# Chuáº©n chá»‰ lÃ  nÃªn partition theo nhÃ³m (Bucketing) hoáº·c chá»¯ cÃ¡i Ä‘áº§u.\n",
    "artist_features.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"total_playlist_appearances\") \\\n",
    "    .parquet(\"s3a://warehouse/gold/artist_features\")\n",
    "\n",
    "# 2. LÆ°u Playlist Features (Partition by Range vÃ­ dá»¥ theo Ä‘á»™ dÃ i playlist)\n",
    "# Táº¡o bucket cá»™t playlist_length Ä‘á»ƒ partition cho gá»n\n",
    "playlist_features_bucketed = playlist_features.withColumn(\n",
    "    \"length_bucket\", \n",
    "    (col(\"playlist_length\") / 50).cast(\"integer\") * 50 # Gom nhÃ³m 0-50, 50-100...\n",
    ")\n",
    "\n",
    "playlist_features_bucketed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"length_bucket\") \\\n",
    "    .parquet(\"s3a://warehouse/gold/playlist_features\")\n",
    "\n",
    "print(\"All Data Saved & Partitioned Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9731b87f-678a-46e1-adcd-e1e7132fe158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Spark Session Ready!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, col\n",
    "\n",
    "# --- 1. KHá»I Táº O SPARK (Cáº¥u hÃ¬nh chuáº©n cho Docker/MinIO) ---\n",
    "# KhÃ´ng cáº§n thay Ä‘á»•i gÃ¬ á»Ÿ Ä‘Ã¢y, Ä‘Ã¢y lÃ  \"chÃ¬a khÃ³a\" Ä‘á»ƒ cháº¡y trong Docker\n",
    "JAR_PATH = \"/home/jovyan/jars\"\n",
    "jars = [\n",
    "    f\"{JAR_PATH}/hadoop-aws-3.3.4.jar\",\n",
    "    f\"{JAR_PATH}/aws-java-sdk-bundle-1.12.262.jar\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify ETL Exact Logic\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"âœ… Spark Session Ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f93d63c-ca82-4461-8b9c-b87ea6cd7c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Scanning MinIO files...\n",
      "ğŸ“‚ TÃ¬m tháº¥y 100 files. Báº¯t Ä‘áº§u xá»­ lÃ½...\n",
      "ğŸ”„ [1/100] Processing: mpd.slice.114000-114999.json ... âŒ Error: Can not create a Path from an empty string\n",
      "ğŸ”„ [2/100] Processing: mpd.slice.115000-115999.json ... âœ… OK\n",
      "ğŸ”„ [3/100] Processing: mpd.slice.117000-117999.json ... âœ… OK\n",
      "ğŸ”„ [4/100] Processing: mpd.slice.124000-124999.json ... âœ… OK\n",
      "ğŸ”„ [5/100] Processing: mpd.slice.126000-126999.json ... âœ… OK\n",
      "ğŸ”„ [6/100] Processing: mpd.slice.127000-127999.json ... âœ… OK\n",
      "ğŸ”„ [7/100] Processing: mpd.slice.191000-191999.json ... âœ… OK\n",
      "ğŸ”„ [8/100] Processing: mpd.slice.192000-192999.json ... âœ… OK\n",
      "ğŸ”„ [9/100] Processing: mpd.slice.193000-193999.json ... âœ… OK\n",
      "ğŸ”„ [10/100] Processing: mpd.slice.214000-214999.json ... âœ… OK\n",
      "ğŸ”„ [11/100] Processing: mpd.slice.216000-216999.json ... âœ… OK\n",
      "ğŸ”„ [12/100] Processing: mpd.slice.217000-217999.json ... âœ… OK\n",
      "ğŸ”„ [13/100] Processing: mpd.slice.22000-22999.json ... âœ… OK\n",
      "ğŸ”„ [14/100] Processing: mpd.slice.224000-224999.json ... âœ… OK\n",
      "ğŸ”„ [15/100] Processing: mpd.slice.225000-225999.json ... âœ… OK\n",
      "ğŸ”„ [16/100] Processing: mpd.slice.227000-227999.json ... âœ… OK\n",
      "ğŸ”„ [17/100] Processing: mpd.slice.290000-290999.json ... âœ… OK\n",
      "ğŸ”„ [18/100] Processing: mpd.slice.291000-291999.json ... âœ… OK\n",
      "ğŸ”„ [19/100] Processing: mpd.slice.292000-292999.json ... âœ… OK\n",
      "ğŸ”„ [20/100] Processing: mpd.slice.3000-3999.json ... âœ… OK\n",
      "ğŸ”„ [21/100] Processing: mpd.slice.315000-315999.json ... âœ… OK\n",
      "ğŸ”„ [22/100] Processing: mpd.slice.316000-316999.json ... âœ… OK\n",
      "ğŸ”„ [23/100] Processing: mpd.slice.317000-317999.json ... âœ… OK\n",
      "ğŸ”„ [24/100] Processing: mpd.slice.324000-324999.json ... âœ… OK\n",
      "ğŸ”„ [25/100] Processing: mpd.slice.325000-325999.json ... âœ… OK\n",
      "ğŸ”„ [26/100] Processing: mpd.slice.326000-326999.json ... âœ… OK\n",
      "ğŸ”„ [27/100] Processing: mpd.slice.38000-38999.json ... âœ… OK\n",
      "ğŸ”„ [28/100] Processing: mpd.slice.390000-390999.json ... âœ… OK\n",
      "ğŸ”„ [29/100] Processing: mpd.slice.391000-391999.json ... âœ… OK\n",
      "ğŸ”„ [30/100] Processing: mpd.slice.393000-393999.json ... âœ… OK\n",
      "ğŸ”„ [31/100] Processing: mpd.slice.410000-410999.json ... âœ… OK\n",
      "ğŸ”„ [32/100] Processing: mpd.slice.411000-411999.json ... âœ… OK\n",
      "ğŸ”„ [33/100] Processing: mpd.slice.413000-413999.json ... âœ… OK\n",
      "ğŸ”„ [34/100] Processing: mpd.slice.420000-420999.json ... âœ… OK\n",
      "ğŸ”„ [35/100] Processing: mpd.slice.422000-422999.json ... âœ… OK\n",
      "ğŸ”„ [36/100] Processing: mpd.slice.423000-423999.json ... âœ… OK\n",
      "ğŸ”„ [37/100] Processing: mpd.slice.448000-448999.json ... âœ… OK\n",
      "ğŸ”„ [38/100] Processing: mpd.slice.449000-449999.json ... âœ… OK\n",
      "ğŸ”„ [39/100] Processing: mpd.slice.479000-479999.json ... âœ… OK\n",
      "ğŸ”„ [40/100] Processing: mpd.slice.495000-495999.json ... âœ… OK\n",
      "ğŸ”„ [41/100] Processing: mpd.slice.496000-496999.json ... âœ… OK\n",
      "ğŸ”„ [42/100] Processing: mpd.slice.497000-497999.json ... âœ… OK\n",
      "ğŸ”„ [43/100] Processing: mpd.slice.51000-51999.json ... âœ… OK\n",
      "ğŸ”„ [44/100] Processing: mpd.slice.510000-510999.json ... âœ… OK\n",
      "ğŸ”„ [45/100] Processing: mpd.slice.511000-511999.json ... âœ… OK\n",
      "ğŸ”„ [46/100] Processing: mpd.slice.512000-512999.json ... âœ… OK\n",
      "ğŸ”„ [47/100] Processing: mpd.slice.521000-521999.json ... âœ… OK\n",
      "ğŸ”„ [48/100] Processing: mpd.slice.522000-522999.json ... âœ… OK\n",
      "ğŸ”„ [49/100] Processing: mpd.slice.523000-523999.json ... âœ… OK\n",
      "ğŸ”„ [50/100] Processing: mpd.slice.54000-54999.json ... âœ… OK\n",
      "ğŸ”„ [51/100] Processing: mpd.slice.548000-548999.json ... âœ… OK\n",
      "ğŸ”„ [52/100] Processing: mpd.slice.549000-549999.json ... âœ… OK\n",
      "ğŸ”„ [53/100] Processing: mpd.slice.578000-578999.json ... âœ… OK\n",
      "ğŸ”„ [54/100] Processing: mpd.slice.594000-594999.json ... âœ… OK\n",
      "ğŸ”„ [55/100] Processing: mpd.slice.596000-596999.json ... âœ… OK\n",
      "ğŸ”„ [56/100] Processing: mpd.slice.597000-597999.json ... âœ… OK\n",
      "ğŸ”„ [57/100] Processing: mpd.slice.611000-611999.json ... âœ… OK\n",
      "ğŸ”„ [58/100] Processing: mpd.slice.612000-612999.json ... âœ… OK\n",
      "ğŸ”„ [59/100] Processing: mpd.slice.613000-613999.json ... âœ… OK\n",
      "ğŸ”„ [60/100] Processing: mpd.slice.620000-620999.json ... âœ… OK\n",
      "ğŸ”„ [61/100] Processing: mpd.slice.621000-621999.json ... âœ… OK\n",
      "ğŸ”„ [62/100] Processing: mpd.slice.622000-622999.json ... âœ… OK\n",
      "ğŸ”„ [63/100] Processing: mpd.slice.678000-678999.json ... âœ… OK\n",
      "ğŸ”„ [64/100] Processing: mpd.slice.679000-679999.json ... âœ… OK\n",
      "ğŸ”„ [65/100] Processing: mpd.slice.694000-694999.json ... âœ… OK\n",
      "ğŸ”„ [66/100] Processing: mpd.slice.695000-695999.json ... âœ… OK\n",
      "ğŸ”„ [67/100] Processing: mpd.slice.697000-697999.json ... âœ… OK\n",
      "ğŸ”„ [68/100] Processing: mpd.slice.7000-7999.json ... âœ… OK\n",
      "ğŸ”„ [69/100] Processing: mpd.slice.70000-70999.json ... âœ… OK\n",
      "ğŸ”„ [70/100] Processing: mpd.slice.710000-710999.json ... âœ… OK\n",
      "ğŸ”„ [71/100] Processing: mpd.slice.712000-712999.json ... âœ… OK\n",
      "ğŸ”„ [72/100] Processing: mpd.slice.713000-713999.json ... âœ… OK\n",
      "ğŸ”„ [73/100] Processing: mpd.slice.720000-720999.json ... âœ… OK\n",
      "ğŸ”„ [74/100] Processing: mpd.slice.721000-721999.json ... âœ… OK\n",
      "ğŸ”„ [75/100] Processing: mpd.slice.723000-723999.json ... âœ… OK\n",
      "ğŸ”„ [76/100] Processing: mpd.slice.749000-749999.json ... âœ… OK\n",
      "ğŸ”„ [77/100] Processing: mpd.slice.75000-75999.json ... âœ… OK\n",
      "ğŸ”„ [78/100] Processing: mpd.slice.778000-778999.json ... âœ… OK\n",
      "ğŸ”„ [79/100] Processing: mpd.slice.779000-779999.json ... âœ… OK\n",
      "ğŸ”„ [80/100] Processing: mpd.slice.794000-794999.json ... âœ… OK\n",
      "ğŸ”„ [81/100] Processing: mpd.slice.795000-795999.json ... âœ… OK\n",
      "ğŸ”„ [82/100] Processing: mpd.slice.796000-796999.json ... âœ… OK\n",
      "ğŸ”„ [83/100] Processing: mpd.slice.81000-81999.json ... âœ… OK\n",
      "ğŸ”„ [84/100] Processing: mpd.slice.84000-84999.json ... âœ… OK\n",
      "ğŸ”„ [85/100] Processing: mpd.slice.844000-844999.json ... âœ… OK\n",
      "ğŸ”„ [86/100] Processing: mpd.slice.846000-846999.json ... âœ… OK\n",
      "ğŸ”„ [87/100] Processing: mpd.slice.847000-847999.json ... âœ… OK\n",
      "ğŸ”„ [88/100] Processing: mpd.slice.874000-874999.json ... âœ… OK\n",
      "ğŸ”„ [89/100] Processing: mpd.slice.875000-875999.json ... âœ… OK\n",
      "ğŸ”„ [90/100] Processing: mpd.slice.877000-877999.json ... âœ… OK\n",
      "ğŸ”„ [91/100] Processing: mpd.slice.898000-898999.json ... âœ… OK\n",
      "ğŸ”„ [92/100] Processing: mpd.slice.899000-899999.json ... âœ… OK\n",
      "ğŸ”„ [93/100] Processing: mpd.slice.945000-945999.json ... âœ… OK\n",
      "ğŸ”„ [94/100] Processing: mpd.slice.946000-946999.json ... âœ… OK\n",
      "ğŸ”„ [95/100] Processing: mpd.slice.947000-947999.json ... âœ… OK\n",
      "ğŸ”„ [96/100] Processing: mpd.slice.974000-974999.json ... âœ… OK\n",
      "ğŸ”„ [97/100] Processing: mpd.slice.975000-975999.json ... âœ… OK\n",
      "ğŸ”„ [98/100] Processing: mpd.slice.976000-976999.json ... âœ… OK\n",
      "ğŸ”„ [99/100] Processing: mpd.slice.998000-998999.json ... âœ… OK\n",
      "ğŸ”„ [100/100] Processing: mpd.slice.999000-999999.json ... âœ… OK\n",
      "\n",
      "ğŸ‰ ETL JOB COMPLETED SUCCESSFULLY!\n",
      "ğŸ” Káº¿t quáº£ máº«u trong MinIO:\n",
      "+------+-------------+---------------------------+------------------------------------+----------------+----------------------+-----------+\n",
      "|pid   |playlist_name|track_name                 |track_uri                           |artist_name     |album_name            |duration_ms|\n",
      "+------+-------------+---------------------------+------------------------------------+----------------+----------------------+-----------+\n",
      "|214000|sept         |Wasted                     |spotify:track:2CAK2t1reUgPK6OMgAMURB|TiÃ«sto          |A Town Called Paradise|190013     |\n",
      "|214000|sept         |Sexy Can I feat. Yung Berg |spotify:track:0DdpxWfVvUGgkJv5536tiF|Ray J           |All I Feel            |204040     |\n",
      "|214000|sept         |Something Just Like This   |spotify:track:6RUKPb4LETWmmr3iAEQktW|The Chainsmokers|Memories...Do Not Open|247160     |\n",
      "|214000|sept         |Broccoli (feat. Lil Yachty)|spotify:track:7yyRTcZmCiyzzJlNzGC9Ol|DRAM            |Big Baby DRAM         |225205     |\n",
      "|214000|sept         |Paris                      |spotify:track:72jbDTw1piOOj770jWNeaG|The Chainsmokers|Memories...Do Not Open|221506     |\n",
      "+------+-------------+---------------------------+------------------------------------+----------------+----------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "# LÆ°u Ã½: KhÃ´ng cáº§n import glob ná»¯a vÃ¬ ta dÃ¹ng Hadoop FS\n",
    "\n",
    "# --- 2. Láº¤Y DANH SÃCH FILE Tá»ª MINIO (CÃCH FIX Lá»–I WRONG FS) ---\n",
    "print(\"ğŸ” Scanning MinIO files...\")\n",
    "\n",
    "# Láº¥y cÃ¡c class Java cáº§n thiáº¿t qua Py4J\n",
    "Path = spark._jvm.org.apache.hadoop.fs.Path\n",
    "URI = spark._jvm.java.net.URI\n",
    "FileSystem = spark._jvm.org.apache.hadoop.fs.FileSystem\n",
    "Configuration = spark._jsc.hadoopConfiguration()\n",
    "\n",
    "# 1. Äá»‹nh nghÄ©a Ä‘Æ°á»ng dáº«n S3\n",
    "bucket_path_str = \"s3a://spotify-raw-data/\"\n",
    "p = Path(bucket_path_str)\n",
    "\n",
    "# 2. Láº¥y FileSystem TÆ¯Æ NG á»¨NG vá»›i Ä‘Æ°á»ng dáº«n Ä‘Ã³ (ÄÃ¢y lÃ  bÆ°á»›c sá»­a lá»—i quan trá»ng)\n",
    "# NÃ³ sáº½ tá»± Ä‘á»™ng nháº­n diá»‡n giao thá»©c s3a:// vÃ  tráº£ vá» S3AFileSystem thay vÃ¬ LocalFileSystem\n",
    "fs = p.getFileSystem(Configuration)\n",
    "\n",
    "# 3. Liá»‡t kÃª file\n",
    "file_statuses = fs.listStatus(p)\n",
    "s3_files = [f.getPath().toString() for f in file_statuses if f.getPath().getName().endswith('.json')]\n",
    "s3_files.sort()\n",
    "\n",
    "print(f\"ğŸ“‚ TÃ¬m tháº¥y {len(s3_files)} files. Báº¯t Ä‘áº§u xá»­ lÃ½...\")\n",
    "\n",
    "# --- 3. VÃ’NG Láº¶P Xá»¬ LÃ (LOGIC Cá»¦A Báº N) ---\n",
    "output_path = \"s3a://spotify-processed-data/\"\n",
    "count = 0\n",
    "\n",
    "for file_path in s3_files:\n",
    "    count += 1\n",
    "    # Láº¥y tÃªn file Ä‘á»ƒ in log cho Ä‘áº¹p\n",
    "    file_name = file_path.split(\"/\")[-1]\n",
    "    \n",
    "    print(f\"ğŸ”„ [{count}/{len(s3_files)}] Processing: {file_name} ...\", end=\" \")\n",
    "    \n",
    "    try:\n",
    "        # 1. Äá»c Data (Read)\n",
    "        # LÆ°u Ã½: file_path láº¥y tá»« Hadoop FS Ä‘Ã£ lÃ  chuáº©n s3a://... rá»“i, dÃ¹ng luÃ´n\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(file_path)\n",
    "        \n",
    "        # Kiá»ƒm tra xem file cÃ³ dá»¯ liá»‡u khÃ´ng\n",
    "        if \"playlists\" in df.columns:\n",
    "            \n",
    "            # --- [LOGIC BIáº¾N Äá»”I Cá»¦A Báº N] ---\n",
    "            df_playlists = df.select(explode(\"playlists\").alias(\"playlist\"))\n",
    "\n",
    "            df_tracks = df_playlists.select(\n",
    "                col(\"playlist.pid\").alias(\"pid\"),\n",
    "                col(\"playlist.name\").alias(\"playlist_name\"),\n",
    "                explode(\"playlist.tracks\").alias(\"track\")\n",
    "            )\n",
    "\n",
    "            final_df = df_tracks.select(\n",
    "                \"pid\",\n",
    "                \"playlist_name\",\n",
    "                col(\"track.track_name\").alias(\"track_name\"),\n",
    "                col(\"track.track_uri\").alias(\"track_uri\"),\n",
    "                col(\"track.artist_name\").alias(\"artist_name\"),\n",
    "                col(\"track.album_name\").alias(\"album_name\"),\n",
    "                col(\"track.duration_ms\").alias(\"duration_ms\")    \n",
    "            )\n",
    "            \n",
    "            # --- [GHI Dá»® LIá»†U] ---\n",
    "            # File Ä‘áº§u tiÃªn thÃ¬ Overwrite (xÃ³a cÅ©), cÃ¡c file sau Append (ná»‘i thÃªm)\n",
    "            mode = \"overwrite\" if count == 1 else \"append\"\n",
    "            final_df.write.mode(mode).parquet(output_path)\n",
    "            \n",
    "            # Dá»n RAM\n",
    "            df.unpersist()\n",
    "            print(\"âœ… OK\")\n",
    "            \n",
    "        else:\n",
    "            print(\"âš ï¸ Skipped (No 'playlists' key)\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "\n",
    "print(\"\\nğŸ‰ ETL JOB COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "# --- 4. KIá»‚M TRA Káº¾T QUáº¢ ---\n",
    "print(\"ğŸ” Káº¿t quáº£ máº«u trong MinIO:\")\n",
    "try:\n",
    "    spark.read.parquet(output_path).show(5, truncate=False)\n",
    "except:\n",
    "    print(\"ChÆ°a cÃ³ dá»¯ liá»‡u Ä‘á»ƒ hiá»ƒn thá»‹.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61c3b041-0fd7-409a-bae9-c368dd9a4813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Äang Ä‘á»c dá»¯ liá»‡u tá»«: s3a://spotify-processed-data/\n",
      "root\n",
      " |-- pid: long (nullable = true)\n",
      " |-- playlist_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_uri: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- album_name: string (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      "\n",
      "ğŸ”„ Äang xá»­ lÃ½ Features...\n",
      "âœ‚ï¸ Äang chia táº­p Train/Test...\n",
      "ğŸ¤– Äang cáº¥u hÃ¬nh Model ALS...\n",
      "ğŸ‹ï¸â€â™‚ï¸ Äang Train Model (CÃ³ thá»ƒ máº¥t vÃ i phÃºt)...\n",
      "âœ… Train xong!\n",
      "ğŸ“‰ Äang Ä‘Ã¡nh giÃ¡ Model...\n",
      "ğŸ¯ Root Mean Squared Error (RMSE) = 0.945421250432738\n",
      "ğŸ’¾ Äang lÆ°u Model xuá»‘ng: s3a://warehouse/models/als_model\n",
      "ğŸ‰ Model training job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# --- 2. LOGIC TRAINING ---\n",
    "def run_model_training_job():\n",
    "    # Sá»­ dá»¥ng dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½ tá»« bÆ°á»›c trÆ°á»›c\n",
    "    # LÆ°u Ã½: Äáº£m báº£o báº¡n Ä‘Ã£ cháº¡y bÆ°á»›c ETL vÃ  cÃ³ dá»¯ liá»‡u táº¡i Ä‘Ã¢y\n",
    "    input_path = \"s3a://spotify-processed-data/\"\n",
    "    \n",
    "    # LÆ°u Model vÃ o bucket warehouse cho cháº¯c Äƒn (trÃ¡nh lá»—i thiáº¿u bucket)\n",
    "    model_output_path = \"s3a://warehouse/models/als_model\"\n",
    "\n",
    "    print(f\"â³ Äang Ä‘á»c dá»¯ liá»‡u tá»«: {input_path}\")\n",
    "    \n",
    "    try:\n",
    "        df = spark.read.parquet(input_path)\n",
    "        \n",
    "        # In schema Ä‘á»ƒ kiá»ƒm tra\n",
    "        df.printSchema()\n",
    "\n",
    "        # Feature Engineer: Táº¡o cá»™t rating giáº£ Ä‘á»‹nh (Implicit Feedback)\n",
    "        # VÃ¬ dá»¯ liá»‡u chá»‰ cÃ³ nghe/khÃ´ng nghe, ta gÃ¡n rating = 1.0 cho má»i tÆ°Æ¡ng tÃ¡c\n",
    "        print(\"ğŸ”„ Äang xá»­ lÃ½ Features...\")\n",
    "        \n",
    "        # Chuyá»ƒn Ä‘á»•i String ID sang Numeric Index (YÃªu cáº§u báº¯t buá»™c cá»§a ALS)\n",
    "        # setHandleInvalid(\"skip\") giÃºp trÃ¡nh lá»—i náº¿u gáº·p ID láº¡\n",
    "        pid_indexer = StringIndexer(inputCol=\"pid\", outputCol=\"pid_numeric\", handleInvalid=\"skip\")\n",
    "        track_indexer = StringIndexer(inputCol=\"track_uri\", outputCol=\"track_numeric\", handleInvalid=\"skip\")\n",
    "        \n",
    "        df_with_rating = df.withColumn(\"rating\", lit(1.0))\n",
    "\n",
    "        # Split Data\n",
    "        print(\"âœ‚ï¸ Äang chia táº­p Train/Test...\")\n",
    "        (training_data, validation_data) = df_with_rating.randomSplit([0.8, 0.2], seed=42)\n",
    "        \n",
    "        # Cache Ä‘á»ƒ cháº¡y nhanh hÆ¡n\n",
    "        training_data.cache()\n",
    "        validation_data.cache()\n",
    "\n",
    "        # Äá»‹nh nghÄ©a Model ALS\n",
    "        print(\"ğŸ¤– Äang cáº¥u hÃ¬nh Model ALS...\")\n",
    "        als = ALS(\n",
    "            userCol=\"pid_numeric\",\n",
    "            itemCol=\"track_numeric\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\", # Quan trá»ng: Bá» qua user/item má»›i khÃ´ng cÃ³ trong train\n",
    "            implicitPrefs=True,\n",
    "            rank=10,\n",
    "            maxIter=5, # Demo Ä‘á»ƒ 5 cho nhanh (Thá»±c táº¿ cÃ³ thá»ƒ Ä‘á»ƒ 10)\n",
    "            regParam=0.1\n",
    "        )\n",
    "\n",
    "        # Táº¡o Pipeline\n",
    "        # LÆ¯U Ã: ÄÃ£ sá»­a lá»—i chÃ­nh táº£ 'all' thÃ nh 'als' á»Ÿ Ä‘Ã¢y\n",
    "        pipeline = Pipeline(stages=[pid_indexer, track_indexer, als])\n",
    "\n",
    "        # Train Model\n",
    "        print(\"ğŸ‹ï¸â€â™‚ï¸ Äang Train Model (CÃ³ thá»ƒ máº¥t vÃ i phÃºt)...\")\n",
    "        model = pipeline.fit(training_data)\n",
    "        print(\"âœ… Train xong!\")\n",
    "\n",
    "        # Evaluate\n",
    "        print(\"ğŸ“‰ Äang Ä‘Ã¡nh giÃ¡ Model...\")\n",
    "        predictions = model.transform(validation_data)\n",
    "        \n",
    "        evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "        rmse = evaluator.evaluate(predictions)\n",
    "        print(f\"ğŸ¯ Root Mean Squared Error (RMSE) = {rmse}\")\n",
    "\n",
    "        # Save Model\n",
    "        print(f\"ğŸ’¾ Äang lÆ°u Model xuá»‘ng: {model_output_path}\")\n",
    "        model.write().overwrite().save(model_output_path)\n",
    "\n",
    "        print(\"ğŸ‰ Model training job completed successfully!\")\n",
    "        \n",
    "        # Unpersist Ä‘á»ƒ giáº£i phÃ³ng RAM\n",
    "        training_data.unpersist()\n",
    "        validation_data.unpersist()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Lá»—i: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# --- 3. CHáº Y JOB ---\n",
    "run_model_training_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a620582-d5ec-400b-a3d6-827ce00ab8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting redis\n",
      "  Downloading redis-7.1.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading redis-7.1.0-py3-none-any.whl (354 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m354.2/354.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hInstalling collected packages: redis\n",
      "Successfully installed redis-7.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a8e4895-4185-4768-9a5c-baebe25c636a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Loading Model...\n",
      "âœ‚ï¸ Láº¥y máº«u 1% danh sÃ¡ch Playlist Ä‘á»ƒ tÃ­nh toÃ¡n...\n",
      "ğŸ¤– Äang táº¡o gá»£i Ã½ cho 100 playlist máº«u...\n",
      "ğŸ”„ Mapping IDs...\n",
      "ğŸ‘€ Káº¿t quáº£ máº«u:\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|pid   |recommendations                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|621832|[spotify:track:6eT7xZZlB2mwyzJ2sUKG6w, spotify:track:2d8JP84HNLKhmd6IYOoupQ, spotify:track:2FBUoWkIuQXwayw2RNo5l6, spotify:track:25khomWgBVamSdKw7hzm3l, spotify:track:1xznGGDReH1oQq0xzbwXa3, spotify:track:2KpCpk6HjXXLb7nnXoXA5O, spotify:track:7yyRTcZmCiyzzJlNzGC9Ol, spotify:track:62vpWI1CHwFy7tMIcSStl8, spotify:track:4w7yrP4RAeeyhfG9nJqQvS, spotify:track:27GmP9AWRs744SzKcpJsTZ]|\n",
      "|695077|[spotify:track:0QsvXIfqM0zZoerQfsI9lm, spotify:track:4kbj5MwxO1bq9wjT5g9HaA, spotify:track:4WjH9Bzt3kx7z8kl0awxh4, spotify:track:0XUfyU2QviPAs6bxSpXYG4, spotify:track:5dNfHmqgr128gMY2tc5CeJ, spotify:track:7BKLCZ1jbUBVqRi2FVlTVw, spotify:track:32OlwWuMpZ6b0aN2RZOeMS, spotify:track:69bp2EbF7Q2rqc5N3ylezZ, spotify:track:5Q0Nhxo0l2bP3pNjpGJwV1, spotify:track:6O6M7pJLABmfBRoGZMu76Y]|\n",
      "|697505|[spotify:track:4scpF6J5uMBvoh6sFB7EL1, spotify:track:4Km5HrUvYTaSUfiSGPJeQR, spotify:track:1wHZx0LgzFHyeIZkUydNXq, spotify:track:2KpCpk6HjXXLb7nnXoXA5O, spotify:track:7yyRTcZmCiyzzJlNzGC9Ol, spotify:track:439TlnnznSiBbQbgXiBqAd, spotify:track:62vpWI1CHwFy7tMIcSStl8, spotify:track:4X5f3vT8MRuXF68pfjNte5, spotify:track:4w7yrP4RAeeyhfG9nJqQvS, spotify:track:27GmP9AWRs744SzKcpJsTZ]|\n",
      "+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "ğŸš€ Äáº©y vÃ o Redis...\n",
      "âœ… Xong! Kiá»ƒm tra Redis.\n",
      "ğŸ” Redis Check Key [playlist:621832]: b'spotify:track:6eT7xZZlB2mwyzJ2sUKG6w,spotify:track:2d8JP84HNLKhmd6IYOoupQ,spotify:track:2FBUoWkIuQXwayw2RNo5l6,spotify:track:25khomWgBVamSdKw7hzm3l,spotify:track:1xznGGDReH1oQq0xzbwXa3,spotify:track:2KpCpk6HjXXLb7nnXoXA5O,spotify:track:7yyRTcZmCiyzzJlNzGC9Ol,spotify:track:62vpWI1CHwFy7tMIcSStl8,spotify:track:4w7yrP4RAeeyhfG9nJqQvS,spotify:track:27GmP9AWRs744SzKcpJsTZ'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import redis\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.sql.functions import col, collect_list, explode\n",
    "# --- 2. HÃ€M GHI VÃ€O REDIS ---\n",
    "# (Thay tháº¿ cho spark_jobs.utils.write_to_redis)\n",
    "# HÃ m ghi Redis (Giá»¯ nguyÃªn)\n",
    "def write_to_redis(iterator):\n",
    "    r = redis.Redis(host='redis', port=6379, db=0)\n",
    "    pipe = r.pipeline()\n",
    "    count = 0\n",
    "    for row in iterator:\n",
    "        key = f\"playlist:{row['pid']}\"\n",
    "        value = \",\".join(row['recommendations'])\n",
    "        pipe.set(key, value)\n",
    "        count += 1\n",
    "        if count % 100 == 0: # Giáº£m batch size xuá»‘ng cho nháº¹\n",
    "            pipe.execute()\n",
    "            count = 0\n",
    "    if count > 0:\n",
    "        pipe.execute()\n",
    "    r.close()\n",
    "\n",
    "# --- 2. LOGIC CHÃNH ---\n",
    "def run_safe_job():\n",
    "    model_path = \"s3a://warehouse/models/als_model\"\n",
    "    \n",
    "    print(f\"â³ Loading Model...\")\n",
    "    try:\n",
    "        model = PipelineModel.load(model_path)\n",
    "    except Exception as e:\n",
    "        print(\"âŒ ChÆ°a cÃ³ model. HÃ£y cháº¡y bÆ°á»›c Train trÆ°á»›c!\")\n",
    "        return\n",
    "\n",
    "    als_model = model.stages[-1]\n",
    "    user_col_name = als_model.getUserCol() # pid_numeric\n",
    "\n",
    "    # === [QUAN TRá»ŒNG] CHá»ˆ Láº¤Y MáºªU USER ===\n",
    "    print(\"âœ‚ï¸ Láº¥y máº«u 1% danh sÃ¡ch Playlist Ä‘á»ƒ tÃ­nh toÃ¡n...\")\n",
    "    \n",
    "    # Láº¥y danh sÃ¡ch ID sá»‘ tá»« metadata mapping Ä‘Ã£ lÆ°u trong model\n",
    "    pid_indexer = model.stages[0]\n",
    "    # Táº¡o DataFrame chá»©a táº¥t cáº£ User ID sá»‘\n",
    "    all_users = spark.range(0, len(pid_indexer.labels)).withColumnRenamed(\"id\", user_col_name)\n",
    "    \n",
    "    # Chá»‰ láº¥y 1% (hoáº·c tá»‘i Ä‘a 100 user) Ä‘á»ƒ demo\n",
    "    subset_users = all_users.sample(fraction=0.01, seed=42).limit(100)\n",
    "    \n",
    "    print(f\"ğŸ¤– Äang táº¡o gá»£i Ã½ cho {subset_users.count()} playlist máº«u...\")\n",
    "    \n",
    "    # Chá»‰ tÃ­nh toÃ¡n cho táº­p con nÃ y (Nháº¹ hÆ¡n gáº¥p trÄƒm láº§n so vá»›i recommendForAllUsers)\n",
    "    recommendations = als_model.recommendForUserSubset(subset_users, 10)\n",
    "\n",
    "    # --- ÃNH Xáº  (MAPPING) ---\n",
    "    print(\"ğŸ”„ Mapping IDs...\")\n",
    "    \n",
    "    pid_labels = pid_indexer.labels\n",
    "    track_labels = model.stages[1].labels\n",
    "\n",
    "    # Táº¡o Map DF\n",
    "    pid_map_df = spark.createDataFrame(list(enumerate(pid_labels)), [\"pid_numeric\", \"pid\"])\n",
    "    track_map_df = spark.createDataFrame(list(enumerate(track_labels)), [\"track_numeric\", \"track_uri\"])\n",
    "\n",
    "    # Join láº¥y PID\n",
    "    recs_with_pid = recommendations.join(pid_map_df, user_col_name, \"inner\") \\\n",
    "        .select(\"pid\", \"recommendations\")\n",
    "\n",
    "    # Explode & Join láº¥y Track URI\n",
    "    exploded = recs_with_pid.select(\"pid\", explode(\"recommendations\").alias(\"rec\")) \\\n",
    "        .select(\"pid\", col(\"rec.\" + als_model.getItemCol()).alias(\"track_numeric\"))\n",
    "\n",
    "    joined_tracks = exploded.join(track_map_df, \"track_numeric\", \"inner\") \\\n",
    "        .select(\"pid\", \"track_uri\")\n",
    "\n",
    "    # Gom láº¡i\n",
    "    final_recs = joined_tracks.groupBy(\"pid\") \\\n",
    "        .agg(collect_list(\"track_uri\").alias(\"recommendations\"))\n",
    "\n",
    "    print(\"ğŸ‘€ Káº¿t quáº£ máº«u:\")\n",
    "    final_recs.show(3, truncate=False)\n",
    "\n",
    "    # --- GHI REDIS ---\n",
    "    print(\"ğŸš€ Äáº©y vÃ o Redis...\")\n",
    "    final_recs.foreachPartition(write_to_redis)\n",
    "    print(\"âœ… Xong! Kiá»ƒm tra Redis.\")\n",
    "    \n",
    "    # Test\n",
    "    r = redis.Redis(host='redis', port=6379, db=0)\n",
    "    if final_recs.count() > 0:\n",
    "        pid = final_recs.first()['pid']\n",
    "        print(f\"ğŸ” Redis Check Key [playlist:{pid}]: {r.get(f'playlist:{pid}')}\")\n",
    "\n",
    "run_safe_job()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "027a02f0-87b3-4274-b474-a8c582968bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b09e29a-7465-4344-8823-7246fa91eec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Redis Check Key [playlist:]: b'spotify:track:7yq4Qj7cqayVTp3FF9CWbm,spotify:track:3rq5w4bQGigXOfdN30ATJt,spotify:track:1jdNcAD8Ir58RlsdGjJJdx,spotify:track:2ihCaVdNZmnHZWt0fvAM7B,spotify:track:0GO8y8jQk1PkHzS31d699N,spotify:track:6vECYJHxYmm3Ydt3fF01pE,spotify:track:6ZYS6QQxTLsQ6IFXdVx1r4,spotify:track:3n69hLUdIsSa1WlRmjMZlW,spotify:track:2QjOHCTQ1Jl3zawyYOpxh6,spotify:track:7w5cxTEzp1rfV3KCy0Bd5N'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 38332)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "r = redis.Redis(host='redis', port=6379, db=0)\n",
    "print(f\"ğŸ” Redis Check Key [playlist:]: {r.get(f'playlist:{117816}')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8def27cd-c994-48e5-9dd9-4e357e22b781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
