{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108f97fd-84fc-43a7-9b20-2afea526a080",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dae9c39-9e78-4091-b16f-c84932467428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing the MinIO server or file: Expecting value: line 1 column 1 (char 0)\n",
      "Ensure MinIO is running and accessible at http://127.0.0.1:9001.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# The direct URL to the JSON file on your MinIO console\n",
    "DATA_URL = 'http://127.0.0.1:9001/browser/spotify-raw-data/mpd.slice.114000-114999.json'\n",
    "\n",
    "try:\n",
    "    # 1. Fetch the data from the URL\n",
    "    response = requests.get(DATA_URL)\n",
    "    response.raise_for_status() # Check for HTTP errors (4xx or 5xx)\n",
    "\n",
    "    # 2. Parse the JSON content\n",
    "    data = response.json()\n",
    "    \n",
    "    print(\"Data successfully fetched and parsed!\")\n",
    "    \n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error accessing the MinIO server or file: {e}\")\n",
    "    print(\"Ensure MinIO is running and accessible at http://127.0.0.1:9001.\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Error decoding JSON: {e}\")\n",
    "    print(\"The file might not be valid JSON.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4440c498-42d1-41df-b3f5-3e74819498c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: minio in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (7.2.20)\n",
      "Requirement already satisfied: argon2-cffi in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from minio) (25.1.0)\n",
      "Requirement already satisfied: certifi in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from minio) (2025.11.12)\n",
      "Requirement already satisfied: pycryptodome in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from minio) (3.23.0)\n",
      "Requirement already satisfied: typing-extensions in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from minio) (4.15.0)\n",
      "Requirement already satisfied: urllib3 in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from minio) (2.6.1)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from argon2-cffi->minio) (25.1.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/bear/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.23)\n"
     ]
    }
   ],
   "source": [
    "!pip install minio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e53a3a-c79a-46a0-8539-02883ae4797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spotify-processed-data\n",
      "spotify-raw-data\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "from minio import Minio\n",
    "import json\n",
    "\n",
    "# Kh·ªüi t·∫°o client\n",
    "client = Minio(\"127.0.0.1:9000\", access_key=\"minioadmin\", secret_key=\"minioadmin\", secure=False)\n",
    "\n",
    "# T·∫£i file JSON t·ª´ bucket\n",
    "for b in client.list_buckets():\n",
    "    print(b.name)\n",
    "data = client.get_object(\"spotify-raw-data\", \"mpd.slice.114000-114999.json\")\n",
    "\n",
    "# ƒê·ªçc n·ªôi dung JSON\n",
    "playlist_data = json.load(data)\n",
    "print(len(playlist_data[\"playlists\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7bb8b296-1bda-4186-ae72-ad124dbde320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ S·ªë l∆∞·ª£ng playlist: 1000\n",
      "‚úÖ T·ªïng s·ªë track entries: 67230\n",
      "‚úÖ S·ªë l∆∞·ª£ng track duy nh·∫•t: 30369\n",
      "   playlist_id playlist_name           track_name  artist_name  \\\n",
      "0       114000         chill               M√©tele   Buscabulla   \n",
      "1       114000         chill                House     Kindness   \n",
      "2       114000         chill          Polish Girl  Neon Indian   \n",
      "3       114000         chill            New Slang    The Shins   \n",
      "4       114000         chill  White Winter Hymnal  Fleet Foxes   \n",
      "\n",
      "                         album_name  \n",
      "0                        Buscabulla  \n",
      "1  World, You Need a Change of Mind  \n",
      "2                       Era Extra√±a  \n",
      "3                Oh, Inverted World  \n",
      "4                       Fleet Foxes  \n"
     ]
    }
   ],
   "source": [
    "playlists = playlist_data['playlists']\n",
    "records = []\n",
    "\n",
    "for p in playlists:\n",
    "    for t in p['tracks']:\n",
    "        records.append({\n",
    "            'playlist_id': p['pid'],\n",
    "            'playlist_name': p.get('name', ''),\n",
    "            'track_name': t.get('track_name', ''),\n",
    "            'artist_name': t.get('artist_name', ''),\n",
    "            'album_name': t.get('album_name', '')\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "print(\"‚úÖ S·ªë l∆∞·ª£ng playlist:\", df['playlist_id'].nunique())\n",
    "print(\"‚úÖ T·ªïng s·ªë track entries:\", len(df))\n",
    "print(\"‚úÖ S·ªë l∆∞·ª£ng track duy nh·∫•t:\", df['track_name'].nunique())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8088929-ccde-4100-93b2-e76737809ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S·ªë b√†i h√°t trung b√¨nh m·ªói playlist: 67.23\n",
      "\n",
      "Top 10 ngh·ªá sƒ© ph·ªï bi·∫øn nh·∫•t:\n",
      "artist_name\n",
      "Drake             729\n",
      "Ed Sheeran        440\n",
      "Kendrick Lamar    418\n",
      "The Weeknd        410\n",
      "Beyonc√©           377\n",
      "Kanye West        361\n",
      "Rihanna           308\n",
      "J. Cole           306\n",
      "Justin Bieber     304\n",
      "Eminem            280\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top 10 album ph·ªï bi·∫øn nh·∫•t:\n",
      "album_name\n",
      "Views                        191\n",
      "Coloring Book                177\n",
      "Stoney                       172\n",
      "DAMN.                        171\n",
      "Purpose                      168\n",
      "Starboy                      165\n",
      "x                            161\n",
      "√∑                            157\n",
      "Beauty Behind The Madness    147\n",
      "Greatest Hits                147\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "avg_tracks = df.groupby('playlist_id')['track_name'].count().mean()\n",
    "print(f\"S·ªë b√†i h√°t trung b√¨nh m·ªói playlist: {avg_tracks:.2f}\")\n",
    "\n",
    "# Ngh·ªá sƒ© ph·ªï bi·∫øn nh·∫•t\n",
    "top_artists = df['artist_name'].value_counts().head(10)\n",
    "print(\"\\nTop 10 ngh·ªá sƒ© ph·ªï bi·∫øn nh·∫•t:\")\n",
    "print(top_artists)\n",
    "\n",
    "# Album ph·ªï bi·∫øn nh·∫•t\n",
    "top_albums = df['album_name'].value_counts().head(10)\n",
    "print(\"\\nTop 10 album ph·ªï bi·∫øn nh·∫•t:\")\n",
    "print(top_albums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24573f06-23bf-46d7-89b0-ae63f5660f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 14:11:56 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spotify Production Pipeline\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.hadoop:hadoop-aws:3.3.4,\"\n",
    "            \"com.amazonaws:aws-java-sdk-bundle:1.12.262\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.timeout\", \"600000\")  \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.establish.timeout\", \"600000\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.attempts.maximum\", \"100\")\\\n",
    "    .config(\"spark.hadoop.fs.s3a.retry.limit\", \"20\")\\\n",
    "    .config(\"spark.sql.caseSensitive\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "print(\"Spark Session Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f118717-b707-4d73-94e9-46ba501b6ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Raw Data from Data Lake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/11 14:11:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/12/11 14:11:59 WARN FileSystem: Failed to initialize filesystem s3a://spotify-raw-data/mpd.slice.114000-114999.json: java.lang.NumberFormatException: For input string: \"60s\"\n",
      "25/12/11 14:11:59 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://spotify-raw-data/mpd.slice.114000-114999.json.\n",
      "java.lang.NumberFormatException: For input string: \"60s\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:709)\n",
      "\tat java.base/java.lang.Long.parseLong(Long.java:832)\n",
      "\tat org.apache.hadoop.conf.Configuration.getLong(Configuration.java:1607)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AUtils.longOption(S3AUtils.java:1024)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initThreadPools(S3AFileSystem.java:719)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.initialize(S3AFileSystem.java:498)\n",
      "\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3615)\n",
      "\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:172)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3716)\n",
      "\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3667)\n",
      "\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:557)\n",
      "\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:366)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:55)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:296)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.json(DataFrameReader.scala:150)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "25/12/11 14:11:59 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "25/12/11 14:11:59 WARN FileSystem: Failed to initialize filesystem s3a://spotify-raw-data/mpd.slice.114000-114999.json: java.lang.NumberFormatException: For input string: \"60s\"\n"
     ]
    },
    {
     "ename": "NumberFormatException",
     "evalue": "For input string: \"60s\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNumberFormatException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading Raw Data from Data Lake...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ƒê·ªçc d·ªØ li·ªáu v·ªõi schema ƒë√£ ƒë·ªãnh nghƒ©a\u001b[39;00m\n\u001b[32m     39\u001b[39m raw_df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_schema\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmode\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mPERMISSIVE\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmultiline\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms3a://spotify-raw-data/mpd.slice.114000-114999.json\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Demo 1 file\u001b[39;00m\n\u001b[32m     45\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRaw Data Loaded.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages/pyspark/sql/readwriter.py:468\u001b[39m, in \u001b[36mDataFrameReader.json\u001b[39m\u001b[34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding, locale, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, allowNonNumericNumbers, useUnsafeRow)\u001b[39m\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) == \u001b[38;5;28mlist\u001b[39m:\n\u001b[32m    467\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._spark._sc._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonUtils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[32m    471\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyspark\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Workspace/big_data/spotify_trends/venv/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mNumberFormatException\u001b[39m: For input string: \"60s\""
     ]
    }
   ],
   "source": [
    "# 1. ƒê·ªãnh nghƒ©a Schema chu·∫©n c·ªßa Spotify MPD\n",
    "# Vi·ªác n√†y gi√∫p Spark ƒë·ªçc nhanh h∆°n v√† lo·∫°i b·ªè b·∫£n ghi sai format ngay t·ª´ ƒë·∫ßu\n",
    "track_schema = StructType([\n",
    "    StructField(\"pos\", IntegerType(), True),\n",
    "    StructField(\"artist_name\", StringType(), True),\n",
    "    StructField(\"track_uri\", StringType(), True),\n",
    "    StructField(\"artist_uri\", StringType(), True),\n",
    "    StructField(\"track_name\", StringType(), True),\n",
    "    StructField(\"album_uri\", StringType(), True),\n",
    "    StructField(\"duration_ms\", LongType(), True),\n",
    "    StructField(\"album_name\", StringType(), True)\n",
    "])\n",
    "\n",
    "playlist_schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"collaborative\", StringType(), True),\n",
    "    StructField(\"pid\", LongType(), True),\n",
    "    StructField(\"modified_at\", LongType(), True),\n",
    "    StructField(\"num_tracks\", IntegerType(), True),\n",
    "    StructField(\"num_albums\", IntegerType(), True),\n",
    "    StructField(\"num_followers\", IntegerType(), True),\n",
    "    StructField(\"tracks\", ArrayType(track_schema), True), # Nested Array\n",
    "    StructField(\"description\", StringType(), True)\n",
    "])\n",
    "info_schema = StructType([\n",
    "    StructField(\"tag\", StringType(), True),\n",
    "    StructField(\"generated_on\", LongType(), True),\n",
    "    StructField(\"slice\", StringType(), True),\n",
    "    StructField(\"version\", StringType(), True)\n",
    "])\n",
    "\n",
    "root_schema = StructType([\n",
    "    StructField(\"info\", info_schema, True),\n",
    "    StructField(\"playlists\", ArrayType(playlist_schema), True)\n",
    "])\n",
    "\n",
    "print(\"Loading Raw Data from Data Lake...\")\n",
    "# ƒê·ªçc d·ªØ li·ªáu v·ªõi schema ƒë√£ ƒë·ªãnh nghƒ©a\n",
    "raw_df = spark.read \\\n",
    "    .schema(root_schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"s3a://spotify-raw-data/mpd.slice.114000-114999.json\") # Demo 1 file\n",
    "\n",
    "print(f\"Raw Data Loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1529020b-6e9e-403a-9e8d-ba250c340d3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing Silver Layer...\n",
      "‚úÖ Silver Layer (Cleaned & Normalized) Saved.\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "|playlist_id|   playlist_name|num_tracks|num_followers|      modified_date|           track_uri|      track_name|          artist_uri|        artist_name|duration_ms|artist_partition|\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "|          0|      Throwbacks|        52|            1|2017-04-29 00:00:00|spotify:track:2eJ...|            Baby|spotify:artist:1u...|      Justin Bieber|     213973|               J|\n",
      "|          1|Awesome Playlist|        39|            1|2017-09-28 00:00:00|spotify:track:2HH...|Eye of the Tiger|spotify:artist:26...|           Survivor|     243773|               S|\n",
      "|          1|Awesome Playlist|        39|            1|2017-09-28 00:00:00|spotify:track:3nJ...|  Right Hand Man|spotify:artist:6s...|Christopher Jackson|     321696|               C|\n",
      "+-----------+----------------+----------+-------------+-------------------+--------------------+----------------+--------------------+-------------------+-----------+----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Silver Layer...\")\n",
    "\n",
    "# 1. Explode Playlists (T√°ch m·∫£ng playlist th√†nh t·ª´ng d√≤ng)\n",
    "playlists_df = raw_df.select(explode(col(\"playlists\")).alias(\"p\"))\n",
    "\n",
    "# 2. Explode Tracks (T√°ch m·∫£ng tracks l·ªìng b√™n trong playlist)\n",
    "# ƒê√¢y l√† b∆∞·ªõc \"Explode nested track arrays\" trong y√™u c·∫ßu\n",
    "flat_df = playlists_df.select(\n",
    "    col(\"p.pid\").alias(\"playlist_id\"),\n",
    "    col(\"p.name\").alias(\"playlist_name\"),\n",
    "    col(\"p.num_tracks\"),\n",
    "    col(\"p.num_followers\"),\n",
    "    col(\"p.modified_at\"),\n",
    "    explode(col(\"p.tracks\")).alias(\"t\")\n",
    ")\n",
    "\n",
    "# 3. Parse & Normalize Metadata (L√†m s·∫°ch)\n",
    "# - Trim: C·∫Øt kho·∫£ng tr·∫Øng th·ª´a\n",
    "# - Lower: Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng ƒë·ªÉ chu·∫©n h√≥a\n",
    "# - Cast: ƒê·∫£m b·∫£o ƒë√∫ng ki·ªÉu d·ªØ li·ªáu\n",
    "silver_df = flat_df.select(\n",
    "    col(\"playlist_id\"),\n",
    "    trim(col(\"playlist_name\")).alias(\"playlist_name\"),\n",
    "    col(\"num_tracks\"),\n",
    "    col(\"num_followers\"),\n",
    "    # Convert timestamp (modified_at l√† epoch seconds)\n",
    "    to_timestamp(col(\"modified_at\")).alias(\"modified_date\"),\n",
    "    \n",
    "    # Track Metadata Parsing\n",
    "    trim(col(\"t.track_uri\")).alias(\"track_uri\"),\n",
    "    trim(col(\"t.track_name\")).alias(\"track_name\"),\n",
    "    trim(col(\"t.artist_uri\")).alias(\"artist_uri\"),\n",
    "    trim(col(\"t.artist_name\")).alias(\"artist_name\"),\n",
    "    col(\"t.duration_ms\"),\n",
    "    \n",
    "    # Th√™m c·ªôt partition key (V√≠ d·ª•: l·∫•y ch·ªØ c√°i ƒë·∫ßu c·ªßa artist ƒë·ªÉ partition cho ƒë·ªÅu)\n",
    "    substring(col(\"t.artist_name\"), 0, 1).alias(\"artist_partition\")\n",
    ").drop_duplicates() # Lo·∫°i b·ªè d√≤ng tr√πng l·∫∑p (Deduplication)\n",
    "\n",
    "# L∆∞u b·∫£ng s·∫°ch n√†y xu·ªëng Data Lake (Silver)\n",
    "silver_df.write.mode(\"overwrite\").parquet(\"s3a://warehouse/silver/tracks\")\n",
    "print(\"Silver Layer (Cleaned & Normalized) Saved.\")\n",
    "silver_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfe64d1-9d2d-402f-979e-f62e020a2a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Gold Layer (Feature Engineering)...\n",
      "Sample Artist Features:\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "|     artist_name|total_playlist_appearances|total_tracks_count|avg_track_duration|total_listen_time_ms|  popularity_score|\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "|           Drake|                       202|               923|237832.82665222103|           219519699|418.29999999999995|\n",
      "|         Rihanna|                       170|               348| 224016.5459770115|            77957758|223.39999999999998|\n",
      "|      Kanye West|                       149|               412|248709.39077669903|           102468269|227.89999999999998|\n",
      "|      The Weeknd|                       139|               291| 264382.6632302406|            76935355|             184.6|\n",
      "|The Chainsmokers|                       121|               229|223965.34934497817|            51288065|153.39999999999998|\n",
      "+----------------+--------------------------+------------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Sample Playlist Features:\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "|playlist_id|       playlist_name|playlist_length|unique_artists|avg_track_duration|diversity_ratio|\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "|        496|songs to sing in ...|             20|            20|          221572.7|            1.0|\n",
      "|        795|        Shape of You|              9|             9|212679.33333333334|            1.0|\n",
      "|        461|         BUST A MOVE|             18|            18|243411.44444444444|            1.0|\n",
      "|        922|         Baby shower|             12|            12|206043.83333333334|            1.0|\n",
      "|        641|               sleep|              8|             8|          274936.5|            1.0|\n",
      "+-----------+--------------------+---------------+--------------+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Processing Gold Layer (Feature Engineering)...\")\n",
    "\n",
    "# Feature 1: Artist Frequency (ƒê·ªô ph·ªï bi·∫øn c·ªßa ngh·ªá sƒ©)\n",
    "# Ngh·ªá sƒ© n√†y xu·∫•t hi·ªán trong bao nhi√™u playlist? T·ªïng th·ªùi l∆∞·ª£ng nghe l√† bao nhi√™u?\n",
    "artist_features = silver_df.groupBy(\"artist_name\") \\\n",
    "    .agg(\n",
    "        countDistinct(\"playlist_id\").alias(\"total_playlist_appearances\"),\n",
    "        count(\"track_uri\").alias(\"total_tracks_count\"),\n",
    "        avg(\"duration_ms\").alias(\"avg_track_duration\"),\n",
    "        sum(\"duration_ms\").alias(\"total_listen_time_ms\")\n",
    "    ) \\\n",
    "    .withColumn(\"popularity_score\", col(\"total_playlist_appearances\") * 0.7 + col(\"total_tracks_count\") * 0.3)\n",
    "\n",
    "# Feature 2: Playlist Complexity (ƒê·ªô ph·ª©c t·∫°p c·ªßa Playlist)\n",
    "# Playlist n√†y c√≥ bao nhi√™u ngh·ªá sƒ© kh√°c nhau? ƒê·ªô ƒëa d·∫°ng th·∫ø n√†o?\n",
    "playlist_features = silver_df.groupBy(\"playlist_id\", \"playlist_name\") \\\n",
    "    .agg(\n",
    "        count(\"track_uri\").alias(\"playlist_length\"),\n",
    "        countDistinct(\"artist_name\").alias(\"unique_artists\"),\n",
    "        avg(\"duration_ms\").alias(\"avg_track_duration\")\n",
    "    ) \\\n",
    "    .withColumn(\"diversity_ratio\", col(\"unique_artists\") / col(\"playlist_length\"))\n",
    "\n",
    "print(\"Sample Artist Features:\")\n",
    "artist_features.orderBy(desc(\"total_playlist_appearances\")).show(5)\n",
    "\n",
    "print(\"Sample Playlist Features:\")\n",
    "playlist_features.orderBy(desc(\"diversity_ratio\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae07444-927e-48e4-b642-81bca65671b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Final Data (Storage Optimization)...\n",
      "All Data Saved & Partitioned Successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving Final Data (Storage Optimization)...\")\n",
    "\n",
    "# 1. L∆∞u Artist Features (Partition theo ch·ªØ c√°i ƒë·∫ßu ƒë·ªÉ truy v·∫•n nhanh)\n",
    "# L∆∞u √Ω: Partition by artist_id/name tr·ª±c ti·∫øp s·∫Ω t·∫°o ra h√†ng tri·ªáu folder nh·ªè (Bad Practice).\n",
    "# Chu·∫©n ch·ªâ l√† n√™n partition theo nh√≥m (Bucketing) ho·∫∑c ch·ªØ c√°i ƒë·∫ßu.\n",
    "artist_features.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"total_playlist_appearances\") \\\n",
    "    .parquet(\"s3a://warehouse/gold/artist_features\")\n",
    "\n",
    "# 2. L∆∞u Playlist Features (Partition by Range v√≠ d·ª• theo ƒë·ªô d√†i playlist)\n",
    "# T·∫°o bucket c·ªôt playlist_length ƒë·ªÉ partition cho g·ªçn\n",
    "playlist_features_bucketed = playlist_features.withColumn(\n",
    "    \"length_bucket\", \n",
    "    (col(\"playlist_length\") / 50).cast(\"integer\") * 50 # Gom nh√≥m 0-50, 50-100...\n",
    ")\n",
    "\n",
    "playlist_features_bucketed.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"length_bucket\") \\\n",
    "    .parquet(\"s3a://warehouse/gold/playlist_features\")\n",
    "\n",
    "print(\"All Data Saved & Partitioned Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f93d63c-ca82-4461-8b9c-b87ea6cd7c3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
