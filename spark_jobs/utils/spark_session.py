import os
import sys
from pathlib import Path
from dotenv import load_dotenv
from pyspark.sql import SparkSession

def create_spark_session(app_name="SpotifyETL"):
    """
    Create and config Spark session with MinIO settings and Local JARs.
    """
    # 1. X√°c ƒë·ªãnh ƒë∆∞·ªùng d·∫´n Project Root v√† th∆∞ m·ª•c JARs
    # C·∫•u tr√∫c: project_root/spark_jobs/utils/spark_session.py -> ƒëi ng∆∞·ª£c l√™n 3 c·∫•p
    current_file = Path(__file__).resolve()
    project_root = current_file.parent.parent.parent
    dotenv_path = project_root / '.env'
    jars_dir = project_root / "jars"

    # 2. Load bi·∫øn m√¥i tr∆∞·ªùng
    load_dotenv(dotenv_path=dotenv_path)
    
    access_key = os.getenv("MINIO_ACCESS_KEY", "minioadmin") # Fallback m·∫∑c ƒë·ªãnh
    secret_key = os.getenv("MINIO_SECRET_KEY", "minioadmin")
    
    print(f"\n=== Configuring Spark Session ===")
    print(f"üìÇ Project Root: {project_root}")
    print(f"üìÇ Jars Directory: {jars_dir}")
    
    # 3. L·∫•y danh s√°ch file JAR trong th∆∞ m·ª•c jars/
    # ƒê√¢y l√† b∆∞·ªõc QUAN TR·ªåNG NH·∫§T ƒë·ªÉ fix l·ªói BulkDelete
    jar_files = []
    if jars_dir.exists():
        jar_files = [str(f) for f in jars_dir.glob("*.jar")]
        print(f"‚úÖ Found {len(jar_files)} JAR files locally.")
    else:
        print(f"‚ö†Ô∏è WARNING: Jars directory not found at {jars_dir}")
        # N·∫øu kh√¥ng th·∫•y folder jars, code s·∫Ω ch·∫°y b·∫±ng th∆∞ vi·ªán m·∫∑c ƒë·ªãnh (d·ªÖ l·ªói)

    # N·ªëi danh s√°ch file th√†nh chu·ªói, c√°ch nhau b·ªüi d·∫•u ph·∫©y
    jars_conf = ",".join(jar_files)

    # 4. Kh·ªüi t·∫°o Spark Session
    builder = SparkSession.builder \
        .appName(app_name) \
        .config("spark.driver.memory", "4g") \
        .config("spark.hadoop.fs.s3a.endpoint", "http://localhost:9000") \
        .config("spark.hadoop.fs.s3a.access.key", access_key) \
        .config("spark.hadoop.fs.s3a.secret.key", secret_key) \
        .config("spark.hadoop.fs.s3a.path.style.access", "true") \
        .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
        .config("spark.hadoop.fs.s3a.aws.credentials.provider", "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider") \
        .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
        .config("spark.jars", jars_conf)  # <--- D√íNG QUAN TR·ªåNG: √âp d√πng JAR local

    # N·∫øu kh√¥ng ch·∫°y tr√™n Kubernetes, set master l√† local
    if "KUBERNETES_SERVICE_HOST" not in os.environ:
        builder = builder.master("local[*]")

    spark = builder.getOrCreate()
    
    # 5. Verify config (Optional but good for debug)
    hadoop_conf = spark.sparkContext._jsc.hadoopConfiguration()
    print("\n=== Verifying Hadoop Configuration ===")
    print(f"fs.s3a.endpoint: {hadoop_conf.get('fs.s3a.endpoint')}")
    print(f"Spark Jars Loaded: {len(jar_files)} files")
    print("="*50 + "\n")
    
    return spark