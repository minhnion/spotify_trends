import sys
import os
import psycopg2
from pathlib import Path
from urllib.parse import urlparse, unquote
from pyspark.sql.functions import col

# Add project root to Python path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from spark_jobs.utils.spark_session import create_spark_session
from dotenv import load_dotenv

def run_db_population_job():
    """
    Read processed data from MinIO and write to PostgreSQL (Supabase).
    Synchronized with Jupyter Notebook logic.
    """
    print("\n" + "="*80)
    print("STARTING DB POPULATION JOB")
    print("="*80 + "\n")
    
    # 1. LOAD CONFIG
    load_dotenv()
    database_url = os.getenv("DATABASE_URL")
    if not database_url:
        raise ValueError("DATABASE_URL must be set in .env file")

    parsed = urlparse(database_url)
    
    # [FIX 1] Th√™m prepareThreshold=0 ƒë·ªÉ tr√°nh l·ªói PgBouncer c·ªßa Supabase
    jdbc_url = f"jdbc:postgresql://{parsed.hostname}:{parsed.port}{parsed.path}?prepareThreshold=0"
    
    connection_properties = {
        "user": parsed.username,
        "password": unquote(parsed.password),
        "driver": "org.postgresql.Driver",
        "sslmode": "require"
    }

    # 2. CLEAN UP OLD DATA (TRUNCATE)
    # Ph·∫ßn n√†y ph·∫£i ch·∫°y tr∆∞·ªõc khi Spark kh·ªüi ƒë·ªông ghi ƒë·ªÉ tr√°nh l·ªói kh√≥a ngo·∫°i
    print("üßπ Cleaning up old data in Database...")
    try:
        conn = psycopg2.connect(
            host=parsed.hostname, 
            port=parsed.port, 
            database=parsed.path[1:], # B·ªè d·∫•u /
            user=parsed.username, 
            password=unquote(parsed.password), 
            sslmode='require'
        )
        cur = conn.cursor()
        # [FIX 2] X√≥a theo th·ª© t·ª± v√† Cascade
        cur.execute("TRUNCATE TABLE playlist_tracks, tracks, playlists, albums, artists CASCADE;")
        conn.commit()
        print("‚úÖ Database cleaned successfully!")
        cur.close()
        conn.close()
    except Exception as e:
        print(f"‚ö†Ô∏è Warning during cleanup: {e}")
        print("Continuing with Spark job...")

    # 3. START SPARK
    spark = create_spark_session()
    
    # [FIX 3] ƒê·ªçc t·ª´ ƒë∆∞·ªùng d·∫´n ch·ª©a Full Schema (c√≥ album, uri...)
    input_path = "s3a://warehouse/spotify_full_schema"
    
    print(f"Attempting to read from: {input_path}")
    
    try:
        full_df = spark.read.parquet(input_path)
        row_count = full_df.count()
        print(f"\n‚úì Successfully read {row_count} records from MinIO")
    except Exception as e:
        print(f"\n‚ùå FAILED TO READ FROM MINIO: {e}")
        spark.stop()
        raise

    # H√†m ghi ti·ªán √≠ch
    def write_to_db(df, table_name):
        print(f"\nWriting to table '{table_name}'...")
        try:
            df.write.jdbc(
                url=jdbc_url,
                table=table_name,
                mode="append", # D√πng append v√¨ ƒë√£ Truncate ·ªü tr√™n
                properties=connection_properties
            )
            print(f"  ‚úì Finished writing to '{table_name}'")
        except Exception as e:
            print(f"  ‚ùå Error writing to '{table_name}': {e}")
            raise # D·ª´ng lu√¥n n·∫øu l·ªói ƒë·ªÉ debug

    # --- 4. GHI D·ªÆ LI·ªÜU (THEO TH·ª® T·ª∞ B·∫¢NG CHA -> CON) ---
    
    try:
        # Table 1: Artists
        print("\n=== Processing Artists Table ===")
        artists_df = full_df.select("artist_uri", "artist_name").distinct()
        write_to_db(artists_df, "artists")
        
        # Table 2: Albums (Code c≈© thi·∫øu ph·∫ßn n√†y)
        print("\n=== Processing Albums Table ===")
        albums_df = full_df.select("album_uri", "album_name").distinct()
        write_to_db(albums_df, "albums")
        
        # Table 3: Playlists
        print("\n=== Processing Playlists Table ===")
        # [FIX 4] Alias playlist_id -> pid
        playlists_df = full_df.select(
            col("playlist_id").alias("pid"), 
            col("playlist_name")
        ).distinct()
        write_to_db(playlists_df, "playlists")
        
        # Table 4: Tracks (Ph·ª• thu·ªôc Artist & Album)
        print("\n=== Processing Tracks Table ===")
        tracks_df = full_df.select(
            "track_uri", "track_name", "artist_uri", "album_uri", "duration_ms"
        ).distinct()
        write_to_db(tracks_df, "tracks")
        
        # Table 5: Playlist_Tracks (Ph·ª• thu·ªôc Playlist & Track)
        print("\n=== Processing Playlist_Tracks Table ===")
        playlist_tracks_df = full_df.select(
            col("playlist_id").alias("pid"), 
            col("track_uri")
        ).distinct()
        write_to_db(playlist_tracks_df, "playlist_tracks")
        
        print("\n" + "="*80)
        print("‚úÖ DATABASE POPULATION JOB COMPLETED SUCCESSFULLY!")
        print("="*80 + "\n")
        
    except Exception as e:
        print(f"\n‚ùå Job Failed: {e}")
    finally:
        full_df.unpersist()
        spark.stop()

if __name__ == "__main__":
    run_db_population_job()